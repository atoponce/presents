%PDF-1.4
%“Œ‹ž ReportLab Generated PDF document http://www.reportlab.com
% 'BasicFonts': class PDFDictionary 
1 0 obj
% The standard fonts dictionary
<< /F1 2 0 R
 /F2 3 0 R
 /F3 14 0 R >>
endobj
% 'F1': class PDFType1Font 
2 0 obj
% Font Helvetica
<< /BaseFont /Helvetica
 /Encoding /WinAnsiEncoding
 /Name /F1
 /Subtype /Type1
 /Type /Font >>
endobj
% 'F2': class PDFType1Font 
3 0 obj
% Font Helvetica-Bold
<< /BaseFont /Helvetica-Bold
 /Encoding /WinAnsiEncoding
 /Name /F2
 /Subtype /Type1
 /Type /Font >>
endobj
% 'Annot.NUMBER1': class PDFDictionary 
4 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (mailto:aaron.toponce@gmail.com) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 153.7323
 704.7736
 273.3723
 716.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER2': class PDFDictionary 
5 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://aarontoponce.org/presents/zfs) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 623.7736
 222.7829
 635.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER3': class PDFDictionary 
6 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (mailto:aaron.toponce@gmail.com) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 146.6029
 605.7736
 266.2429
 617.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER4': class PDFDictionary 
7 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://creativecommons.org/licenses/by-sa/3.0/) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 83.26291
 536.7736
 292.2329
 548.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER5': class PDFDictionary 
8 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://creativecommons.org/licenses/by-sa/3.0/legalcode) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 185.7736
 311.6829
 197.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER6': class PDFDictionary 
9 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://ae7.st/s/storage) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 85.69291
 125.7736
 532.5827
 137.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page1': class PDFPage 
10 0 obj
% Page dictionary
<< /Annots [ 4 0 R
 5 0 R
 6 0 R
 7 0 R
 8 0 R
 9 0 R ]
 /Contents 99 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER7': class PDFDictionary 
11 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://dwellman.tumblr.com/) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 287.4129
 642.7736
 409.6729
 654.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page2': class PDFPage 
12 0 obj
% Page dictionary
<< /Annots [ 11 0 R ]
 /Contents 100 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER8': class PDFDictionary 
13 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (https://blogs.oracle.com/bonwick/entry/128_bit_storage_are_you) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 246.8927
 687.7736
 532.5827
 699.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'F3': class PDFType1Font 
14 0 obj
% Font Courier
<< /BaseFont /Courier
 /Encoding /WinAnsiEncoding
 /Name /F3
 /Subtype /Type1
 /Type /Font >>
endobj
% 'Annot.NUMBER9': class PDFDictionary 
15 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2832) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 138.7736
 532.5827
 150.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page3': class PDFPage 
16 0 obj
% Page dictionary
<< /Annots [ 13 0 R
 15 0 R ]
 /Contents 101 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER10': class PDFDictionary 
17 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://zfsonlinux.org) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 85.69291
 525.7736
 532.5827
 537.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER11': class PDFDictionary 
18 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2357) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 402.7736
 532.5827
 414.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page4': class PDFPage 
19 0 obj
% Page dictionary
<< /Annots [ 17 0 R
 18 0 R ]
 /Contents 102 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER12': class PDFDictionary 
20 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2584) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 222.5736
 532.5827
 234.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page5': class PDFPage 
21 0 obj
% Page dictionary
<< /Annots [ 20 0 R ]
 /Contents 103 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Page6': class PDFPage 
22 0 obj
% Page dictionary
<< /Contents 104 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Page7': class PDFPage 
23 0 obj
% Page dictionary
<< /Contents 105 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER13': class PDFDictionary 
24 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2590) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 339.5736
 532.5827
 351.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page8': class PDFPage 
25 0 obj
% Page dictionary
<< /Annots [ 24 0 R ]
 /Contents 106 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER14': class PDFDictionary 
26 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2564) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 184.5736
 178.0529
 196.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER15': class PDFDictionary 
27 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2592) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 197.5129
 184.5736
 310.0929
 196.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page9': class PDFPage 
28 0 obj
% Page dictionary
<< /Annots [ 26 0 R
 27 0 R ]
 /Contents 107 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER16': class PDFDictionary 
29 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2659) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 486.7736
 532.5827
 498.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page10': class PDFPage 
30 0 obj
% Page dictionary
<< /Annots [ 29 0 R ]
 /Contents 108 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER17': class PDFDictionary 
31 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2594) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 485.5736
 532.5827
 497.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER18': class PDFDictionary 
32 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2630) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 158.5736
 532.5827
 170.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page11': class PDFPage 
33 0 obj
% Page dictionary
<< /Annots [ 31 0 R
 32 0 R ]
 /Contents 109 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER19': class PDFDictionary 
34 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2632) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 123.5736
 532.5827
 135.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page12': class PDFPage 
35 0 obj
% Page dictionary
<< /Annots [ 34 0 R ]
 /Contents 110 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Page13': class PDFPage 
36 0 obj
% Page dictionary
<< /Contents 111 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER20': class PDFDictionary 
37 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2782) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 645.7736
 532.5827
 657.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page14': class PDFPage 
38 0 obj
% Page dictionary
<< /Annots [ 37 0 R ]
 /Contents 112 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Page15': class PDFPage 
39 0 obj
% Page dictionary
<< /Contents 113 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER21': class PDFDictionary 
40 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2849) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 108.7736
 532.5827
 120.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page16': class PDFPage 
41 0 obj
% Page dictionary
<< /Annots [ 40 0 R ]
 /Contents 114 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER22': class PDFDictionary 
42 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2878) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 227.1736
 532.5827
 239.1736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page17': class PDFPage 
43 0 obj
% Page dictionary
<< /Annots [ 42 0 R ]
 /Contents 115 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER23': class PDFDictionary 
44 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2878) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 531.5736
 532.5827
 543.5736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER24': class PDFDictionary 
45 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2900) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 135.3736
 532.5827
 147.3736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page18': class PDFPage 
46 0 obj
% Page dictionary
<< /Annots [ 44 0 R
 45 0 R ]
 /Contents 116 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER25': class PDFDictionary 
47 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (mailto:tank/store1@001) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 223.8588
 270.9736
 304.0608
 282.9736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page19': class PDFPage 
48 0 obj
% Page dictionary
<< /Annots [ 47 0 R ]
 /Contents 117 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER26': class PDFDictionary 
49 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2910) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 699.7736
 532.5827
 711.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER27': class PDFDictionary 
50 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2933) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 293.5859
 532.5827
 305.5859 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page20': class PDFPage 
51 0 obj
% Page dictionary
<< /Annots [ 49 0 R
 50 0 R ]
 /Contents 118 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER28': class PDFDictionary 
52 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2943) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 326.3736
 532.5827
 338.3736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page21': class PDFPage 
53 0 obj
% Page dictionary
<< /Annots [ 52 0 R ]
 /Contents 119 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER29': class PDFDictionary 
54 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2950) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 534.3736
 532.5827
 546.3736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page22': class PDFPage 
55 0 obj
% Page dictionary
<< /Annots [ 54 0 R ]
 /Contents 120 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Page23': class PDFPage 
56 0 obj
% Page dictionary
<< /Contents 121 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER30': class PDFDictionary 
57 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/?p=2963) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 62.69291
 735.7736
 532.5827
 747.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page24': class PDFPage 
58 0 obj
% Page dictionary
<< /Annots [ 57 0 R ]
 /Contents 122 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'Annot.NUMBER31': class PDFDictionary 
59 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://pthree.org/category/zfs) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 85.69291
 651.7736
 532.5827
 663.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER32': class PDFDictionary 
60 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (http://aarontoponce.org/presents/zfs) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 85.69291
 633.7736
 532.5827
 645.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Annot.NUMBER33': class PDFDictionary 
61 0 obj
<< /A << /S /URI
 /Type /Action
 /URI (mailto:aaron.toponce@gmail.com) >>
 /Border [ 0
 0
 0 ]
 /Rect [ 85.69291
 615.7736
 532.5827
 627.7736 ]
 /Subtype /Link
 /Type /Annot >>
endobj
% 'Page25': class PDFPage 
62 0 obj
% Page dictionary
<< /Annots [ 59 0 R
 60 0 R
 61 0 R ]
 /Contents 123 0 R
 /MediaBox [ 0
 0
 595.2756
 841.8898 ]
 /Parent 98 0 R
 /Resources << /Font 1 0 R
 /ProcSet [ /PDF
 /Text
 /ImageB
 /ImageC
 /ImageI ] >>
 /Rotate 0
 /Trans <<  >>
 /Type /Page >>
endobj
% 'R63': class PDFCatalog 
63 0 obj
% Document Root
<< /Outlines 65 0 R
 /PageLabels 124 0 R
 /PageMode /UseNone
 /Pages 98 0 R
 /Type /Catalog >>
endobj
% 'R64': class PDFInfo 
64 0 obj
<< /Author (Aaron Toponce)
 /CreationDate (D:20131217181510+07'00')
 /Creator (\(unspecified\))
 /Keywords ()
 /Producer (ReportLab PDF Library - www.reportlab.com)
 /Subject (\(unspecified\))
 /Title (ZFS 1010) >>
endobj
% 'R65': class PDFOutlines 
65 0 obj
<< /Count 32
 /First 66 0 R
 /Last 97 0 R
 /Type /Outlines >>
endobj
% 'Outline.0': class OutlineEntryObject 
66 0 obj
<< /Dest [ 10 0 R
 /XYZ
 62.69291
 674.0236
 0 ]
 /Next 67 0 R
 /Parent 65 0 R
 /Title (Contact and Details) >>
endobj
% 'Outline.1': class OutlineEntryObject 
67 0 obj
<< /Dest [ 10 0 R
 /XYZ
 62.69291
 593.0236
 0 ]
 /Next 68 0 R
 /Parent 65 0 R
 /Prev 66 0 R
 /Title (License) >>
endobj
% 'Outline.2': class OutlineEntryObject 
68 0 obj
<< /Dest [ 10 0 R
 /XYZ
 62.69291
 173.0236
 0 ]
 /Next 69 0 R
 /Parent 65 0 R
 /Prev 67 0 R
 /Title (Motivation) >>
endobj
% 'Outline.3': class OutlineEntryObject 
69 0 obj
<< /Dest [ 12 0 R
 /XYZ
 62.69291
 633.0236
 0 ]
 /Next 70 0 R
 /Parent 65 0 R
 /Prev 68 0 R
 /Title (What is ZFS?) >>
endobj
% 'Outline.4': class OutlineEntryObject 
70 0 obj
<< /Dest [ 16 0 R
 /XYZ
 62.69291
 291.0236
 0 ]
 /Next 71 0 R
 /Parent 65 0 R
 /Prev 69 0 R
 /Title (Copy-on-write) >>
endobj
% 'Outline.5': class OutlineEntryObject 
71 0 obj
<< /Dest [ 19 0 R
 /XYZ
 62.69291
 573.0236
 0 ]
 /Next 72 0 R
 /Parent 65 0 R
 /Prev 70 0 R
 /Title (Installing ZFS on GNU/Linux) >>
endobj
% 'Outline.6': class OutlineEntryObject 
72 0 obj
<< /Dest [ 19 0 R
 /XYZ
 62.69291
 390.0236
 0 ]
 /Next 73 0 R
 /Parent 65 0 R
 /Prev 71 0 R
 /Title (ZFS Commands) >>
endobj
% 'Outline.7': class OutlineEntryObject 
73 0 obj
<< /Dest [ 21 0 R
 /XYZ
 62.69291
 741.0236
 0 ]
 /Next 74 0 R
 /Parent 65 0 R
 /Prev 72 0 R
 /Title (Beginner pool) >>
endobj
% 'Outline.8': class OutlineEntryObject 
74 0 obj
<< /Dest [ 21 0 R
 /XYZ
 62.69291
 428.8236
 0 ]
 /Next 75 0 R
 /Parent 65 0 R
 /Prev 73 0 R
 /Title (VDEVs) >>
endobj
% 'Outline.9': class OutlineEntryObject 
75 0 obj
<< /Dest [ 21 0 R
 /XYZ
 62.69291
 167.8236
 0 ]
 /Next 76 0 R
 /Parent 65 0 R
 /Prev 74 0 R
 /Title (Disk VDEV) >>
endobj
% 'Outline.10': class OutlineEntryObject 
76 0 obj
<< /Dest [ 22 0 R
 /XYZ
 62.69291
 399.8236
 0 ]
 /Next 77 0 R
 /Parent 65 0 R
 /Prev 75 0 R
 /Title (File VDEV) >>
endobj
% 'Outline.11': class OutlineEntryObject 
77 0 obj
<< /Dest [ 23 0 R
 /XYZ
 62.69291
 575.8236
 0 ]
 /Next 78 0 R
 /Parent 65 0 R
 /Prev 76 0 R
 /Title (Mirror VDEV) >>
endobj
% 'Outline.12': class OutlineEntryObject 
78 0 obj
<< /Dest [ 25 0 R
 /XYZ
 62.69291
 491.8236
 0 ]
 /Next 79 0 R
 /Parent 65 0 R
 /Prev 77 0 R
 /Title (RAID-Z VDEVs) >>
endobj
% 'Outline.13': class OutlineEntryObject 
79 0 obj
<< /Dest [ 28 0 R
 /XYZ
 62.69291
 633.0236
 0 ]
 /Next 80 0 R
 /Parent 65 0 R
 /Prev 78 0 R
 /Title (Other VDEVs) >>
endobj
% 'Outline.14': class OutlineEntryObject 
80 0 obj
<< /Dest [ 28 0 R
 /XYZ
 62.69291
 336.8236
 0 ]
 /Next 81 0 R
 /Parent 65 0 R
 /Prev 79 0 R
 /Title (The Separate Intent Log \(SLOG\)) >>
endobj
% 'Outline.15': class OutlineEntryObject 
81 0 obj
<< /Dest [ 30 0 R
 /XYZ
 62.69291
 693.0236
 0 ]
 /Next 82 0 R
 /Parent 65 0 R
 /Prev 80 0 R
 /Title (The Adjustable Replacement Cache) >>
endobj
% 'Outline.16': class OutlineEntryObject 
82 0 obj
<< /Dest [ 30 0 R
 /XYZ
 62.69291
 330.0236
 0 ]
 /Next 83 0 R
 /Parent 65 0 R
 /Prev 81 0 R
 /Title (Hybrid Storage Pools) >>
endobj
% 'Outline.17': class OutlineEntryObject 
83 0 obj
<< /Dest [ 33 0 R
 /XYZ
 62.69291
 583.8236
 0 ]
 /Next 84 0 R
 /Parent 65 0 R
 /Prev 82 0 R
 /Title (Export and Import pools) >>
endobj
% 'Outline.18': class OutlineEntryObject 
84 0 obj
<< /Dest [ 33 0 R
 /XYZ
 62.69291
 292.8236
 0 ]
 /Next 85 0 R
 /Parent 65 0 R
 /Prev 83 0 R
 /Title (Scrub and Resilver pools) >>
endobj
% 'Outline.19': class OutlineEntryObject 
85 0 obj
<< /Dest [ 35 0 R
 /XYZ
 62.69291
 257.8236
 0 ]
 /Next 86 0 R
 /Parent 65 0 R
 /Prev 84 0 R
 /Title (Zpool Properties) >>
endobj
% 'Outline.20': class OutlineEntryObject 
86 0 obj
<< /Dest [ 36 0 R
 /XYZ
 62.69291
 168.6236
 0 ]
 /Next 87 0 R
 /Parent 65 0 R
 /Prev 85 0 R
 /Title (Zpool Best Practices and Caveats) >>
endobj
% 'Outline.21': class OutlineEntryObject 
87 0 obj
<< /Dest [ 41 0 R
 /XYZ
 62.69291
 279.0236
 0 ]
 /Next 88 0 R
 /Parent 65 0 R
 /Prev 86 0 R
 /Title (Creating ZFS Datasets) >>
endobj
% 'Outline.22': class OutlineEntryObject 
88 0 obj
<< /Dest [ 43 0 R
 /XYZ
 62.69291
 343.4236
 0 ]
 /Next 89 0 R
 /Parent 65 0 R
 /Prev 87 0 R
 /Title (Compression) >>
endobj
% 'Outline.23': class OutlineEntryObject 
89 0 obj
<< /Dest [ 46 0 R
 /XYZ
 62.69291
 683.8236
 0 ]
 /Next 90 0 R
 /Parent 65 0 R
 /Prev 88 0 R
 /Title (Deduplication) >>
endobj
% 'Outline.24': class OutlineEntryObject 
90 0 obj
<< /Dest [ 46 0 R
 /XYZ
 62.69291
 305.6236
 0 ]
 /Next 91 0 R
 /Parent 65 0 R
 /Prev 89 0 R
 /Title (Snapshots and Clones) >>
endobj
% 'Outline.25': class OutlineEntryObject 
91 0 obj
<< /Dest [ 48 0 R
 /XYZ
 62.69291
 186.2236
 0 ]
 /Next 92 0 R
 /Parent 65 0 R
 /Prev 90 0 R
 /Title (Sending and Receiving Snapshots) >>
endobj
% 'Outline.26': class OutlineEntryObject 
92 0 obj
<< /Dest [ 51 0 R
 /XYZ
 62.69291
 427.8359
 0 ]
 /Next 93 0 R
 /Parent 65 0 R
 /Prev 91 0 R
 /Title (ZVOLs) >>
endobj
% 'Outline.27': class OutlineEntryObject 
93 0 obj
<< /Dest [ 53 0 R
 /XYZ
 62.69291
 496.6236
 0 ]
 /Next 94 0 R
 /Parent 65 0 R
 /Prev 92 0 R
 /Title (Sharing via NFS, SMB and iSCSI) >>
endobj
% 'Outline.28': class OutlineEntryObject 
94 0 obj
<< /Dest [ 55 0 R
 /XYZ
 62.69291
 650.6236
 0 ]
 /Next 95 0 R
 /Parent 65 0 R
 /Prev 93 0 R
 /Title (Dataset Properties) >>
endobj
% 'Outline.29': class OutlineEntryObject 
95 0 obj
<< /Dest [ 56 0 R
 /XYZ
 62.69291
 163.4236
 0 ]
 /Next 96 0 R
 /Parent 65 0 R
 /Prev 94 0 R
 /Title (Dataset Best Practices and Caveats) >>
endobj
% 'Outline.30': class OutlineEntryObject 
96 0 obj
<< /Dest [ 58 0 R
 /XYZ
 62.69291
 99.02362
 0 ]
 /Next 97 0 R
 /Parent 65 0 R
 /Prev 95 0 R
 /Title (Fin) >>
endobj
% 'Outline.31': class OutlineEntryObject 
97 0 obj
<< /Dest [ 62 0 R
 /XYZ
 62.69291
 699.0236
 0 ]
 /Parent 65 0 R
 /Prev 96 0 R
 /Title (Resources) >>
endobj
% 'R98': class PDFPages 
98 0 obj
% page tree
<< /Count 25
 /Kids [ 10 0 R
 12 0 R
 16 0 R
 19 0 R
 21 0 R
 22 0 R
 23 0 R
 25 0 R
 28 0 R
 30 0 R
 33 0 R
 35 0 R
 36 0 R
 38 0 R
 39 0 R
 41 0 R
 43 0 R
 46 0 R
 48 0 R
 51 0 R
 53 0 R
 55 0 R
 56 0 R
 58 0 R
 62 0 R ]
 /Type /Pages >>
endobj
% 'R99': class PDFStream 
99 0 obj
% page stream
<< /Length 8719 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 4 Tm /F2 20 Tf 24 TL 191.0349 0 Td (ZFS 1010) Tj T* -191.0349 0 Td ET
Q
Q
q
1 0 0 1 62.69291 716.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 36.93937 0 Td (Author:) Tj T* -36.93937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Aaron Toponce) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 701.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 43.02937 0 Td (Email:) Tj T* -43.02937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (aaron.toponce@gmail.com) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 686.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 48.03937 0 Td (Date:) Tj T* -48.03937 0 Td ET
Q
Q
q
1 0 0 1 91.03937 3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Dec 17, 2013) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 653.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Contact and Details) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 623.0236 cm
q
BT 1 0 0 1 0 14 Tm 8.865814 Tw 12 TL /F1 10 Tf 0 0 0 rg (You can find the source code, PDF, compressed tarball and HTML presentation at) Tj T* 0 Tw 0 0 .501961 rg (http://aarontoponce.org/presents/zfs) Tj 0 0 0 rg (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 605.0236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (My email address: ) Tj 0 0 .501961 rg (aaron.toponce@gmail.com) Tj 0 0 0 rg (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 572.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (License) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 554.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (This presentation is licensed under the Creative Commons Attribution-ShareAlike license.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 536.0236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (See ) Tj 0 0 .501961 rg (http://creativecommons.org/licenses/by-sa/3.0/ ) Tj 0 0 0 rg (for more details.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 506.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 5.653555 Tw (This document is licensed under the CC:BY:SA Details to the license can be found here:) Tj T* 0 Tw (http://creativecommons.org/licenses/by-sa/3.0/) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 490.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL (The licnese states the following:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 451.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 26 Tm  T* ET
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (You are free to copy, distribute and tranmit this work.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (You are free to adapt the work.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 435.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL (Under the following conditions:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 384.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 38 Tm  T* ET
q
1 0 0 1 20 42 cm
Q
q
1 0 0 1 20 42 cm
Q
q
1 0 0 1 20 30 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (You must attribute the work to the copyright holder.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.67061 Tw (If you alter, transform, or build on this work, you may redistribute the work under the same,) Tj T* 0 Tw (similar or compatible license.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 368.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL (With the understanding that:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 227.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 128 Tm  T* ET
q
1 0 0 1 20 132 cm
Q
q
1 0 0 1 20 132 cm
Q
q
1 0 0 1 20 120 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Any conditions may be waived if you get written permission from the copyright holder.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 114 cm
Q
q
1 0 0 1 20 30 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (In no way are any of the following rights affected by the license:) Tj T* ET
Q
Q
q
1 0 0 1 23 63 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 60 cm
Q
q
1 0 0 1 20 60 cm
Q
q
1 0 0 1 20 48 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Your fair dealing or fair use rights;) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 42 cm
Q
q
1 0 0 1 20 30 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (The author's moral rights;) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.379985 Tw (Rights other persons may have either in the work itself or in how the work is used,) Tj T* 0 Tw (such as publicity or privacy rights.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .494104 Tw (For any reuse or distribution, you must make clear to others the license terms of this work. The) Tj T* 0 Tw (best way to do this is with a link to the web page provided above or below.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 185.0236 cm
q
BT 1 0 0 1 0 26 Tm .121488 Tw 12 TL /F1 10 Tf 0 0 0 rg (The above is a human-readable summary of the license, and is not to be used as a legal substitute for the) Tj T* 0 Tw 15.97498 Tw (actual licnse. Please refer to the formal legal document provided here:) Tj T* 0 Tw 0 0 .501961 rg (http://creativecommons.org/licenses/by-sa/3.0/legalcode) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 152.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Motivation) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 140.0236 cm
Q
q
1 0 0 1 62.69291 140.0236 cm
Q
q
1 0 0 1 62.69291 128.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://ae7.st/s/storage) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 122.0236 cm
Q
q
1 0 0 1 62.69291 110.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Byte- 1 grain of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 104.0236 cm
Q
q
1 0 0 1 62.69291 92.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (KB- 1 cup of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 86.02362 cm
Q
 
endstream
endobj
% 'R100': class PDFStream 
100 0 obj
% page stream
<< /Length 10848 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (MB- 8 bags of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 747.0236 cm
Q
q
1 0 0 1 62.69291 735.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (GB- 3 53' semi trailers of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 729.0236 cm
Q
q
1 0 0 1 62.69291 717.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (TB- 2 container ships of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 711.0236 cm
Q
q
1 0 0 1 62.69291 699.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (PB- 3" blanket of rice covering Manhattan) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 693.0236 cm
Q
q
1 0 0 1 62.69291 681.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (EB- Blanket of rice covering CA, OR & WA states) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
Q
q
1 0 0 1 62.69291 663.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ZB- Pacific Ocean of rice) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 657.0236 cm
Q
q
1 0 0 1 62.69291 645.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (YB- An Earth sized rice ball \(Dave Wellman - ) Tj 0 0 .501961 rg (http://dwellman.tumblr.com/) Tj 0 0 0 rg (\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 645.0236 cm
Q
q
1 0 0 1 62.69291 612.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (What is ZFS?) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 588.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (128-bit filesystem \(actually, not really\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 582.0236 cm
Q
q
1 0 0 1 62.69291 570.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Copy-on-write filesystem) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 564.0236 cm
Q
q
1 0 0 1 62.69291 552.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Combined filesystem, volume and RAID manager) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 546.0236 cm
Q
q
1 0 0 1 62.69291 534.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Native support for SSDs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 528.0236 cm
Q
q
1 0 0 1 62.69291 516.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Deduplication) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 510.0236 cm
Q
q
1 0 0 1 62.69291 498.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Compression) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 492.0236 cm
Q
q
1 0 0 1 62.69291 480.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Encryption \(not Free Software\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 474.0236 cm
Q
q
1 0 0 1 62.69291 462.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Checksums from top to bottom) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 456.0236 cm
Q
q
1 0 0 1 62.69291 444.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Developed from the ground up) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 444.0236 cm
Q
q
1 0 0 1 62.69291 402.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .434431 Tw (Calling ZFS a 128-bit filesystem is a bit of false advertising. In practice, it's really only a 78-bit filesystem.) Tj T* 0 Tw 2.728221 Tw (Still, this can handle 256 zettabytes. However, this is a far cry from what would be advertized as) Tj T* 0 Tw (288,230,376,151,711,744 zettabytes. The current sizes are as follows:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 396.0236 cm
Q
q
1 0 0 1 62.69291 396.0236 cm
Q
q
1 0 0 1 62.69291 384.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^48 \227 Number of entries in any individual directory) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 378.0236 cm
Q
q
1 0 0 1 62.69291 366.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^64 bytes \227 Maximum size of a single file) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 360.0236 cm
Q
q
1 0 0 1 62.69291 348.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^64 bytes \227 Maximum size of any attribute) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 342.0236 cm
Q
q
1 0 0 1 62.69291 330.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^78 bytes \227 Maximum size of any pool) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 324.0236 cm
Q
q
1 0 0 1 62.69291 300.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .765251 Tw (2^56 \227 Number of attributes of a file \(actually constrained to 2^48 for the number of files in a ZFS) Tj T* 0 Tw (file system\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 294.0236 cm
Q
q
1 0 0 1 62.69291 282.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^64 \227 Number of devices in any pool) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 276.0236 cm
Q
q
1 0 0 1 62.69291 264.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^64 \227 Number of pools in a system) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 258.0236 cm
Q
q
1 0 0 1 62.69291 246.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (2^64 \227 Number of file systems in a pool) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 246.0236 cm
Q
q
1 0 0 1 62.69291 216.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.194985 Tw (Jeff Bonwick, the developer of ZFS makes an interesting observation about what it would take to store) Tj T* 0 Tw (2^128 bits worth of data:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 210.0236 cm
Q
q
1 0 0 1 62.69291 76.86614 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 48 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL 4.14998 Tw (Although we'd all like Moore's Law to continue forever, quantum mechanics imposes some) Tj T* 0 Tw 2.754597 Tw (fundamental limits on the computation rate and information capacity of any physical device. In) Tj T* 0 Tw .042093 Tw (particular, it has been shown that 1 kilogram of matter confined to 1 liter of space can perform at most) Tj T* 0 Tw .577318 Tw (10^51 operations per second on at most 1031 bits of information [see Seth Lloyd, "Ultimate physical) Tj T* 0 Tw 1.01998 Tw (limits to computation." Nature 406, 1047-1054 \(2000\)]. A fully-populated 128-bit storage pool would) Tj T* 0 Tw .017045 Tw (contain 2^128 blocks = 2^137 bytes = 2140 bits; therefore the minimum mass required to hold the bits) Tj T* 0 Tw (would be \(2^140 bits\) / \(10^31 bits/kg\) = 136 billion kg.) Tj T* ET
Q
Q
q
1 0 0 1 20 30 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (That's a lot of gear.) Tj T* ET
Q
Q
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .145777 Tw (To operate at the 10^31 bits/kg limit, however, the entire mass of the computer must be in the form of) Tj T* 0 Tw .957765 Tw (pure energy. By E=mc^2, the rest energy of 136 billion kg is 1.2x10^28 J. The mass of the oceans) Tj T* 0 Tw ET
Q
Q
q
Q
Q
 
endstream
endobj
% 'R101': class PDFStream 
101 0 obj
% page stream
<< /Length 7794 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 687.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 18 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .802988 Tw (is about 1.4x10^21 kg. It takes about 4,000 J to raise the temperature of 1 kg of water by 1 degree) Tj T* 0 Tw .946303 Tw (Celcius, and thus about 400,000 J to heat 1 kg of water from freezing to boiling. The latent heat of) Tj T* 0 Tw 2.534651 Tw (vaporization adds another 2 million J/kg. Thus the energy required to boil the oceans is about) Tj T* 0 Tw 1.575984 Tw (2.4x10^6 J/kg * 1.4x10^21 kg = 3.4x1^027 J. Thus, fully populating a 128-bit storage pool would,) Tj T* 0 Tw (literally, require more energy than boiling the oceans.) Tj T* ET
Q
Q
q
1 0 0 1 20 0 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 164.1998 0 Td (https://blogs.oracle.com/bonwick/entry/128_bit_storage_are_you) Tj T* -164.1998 0 Td ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 687.0236 cm
Q
q
1 0 0 1 62.69291 669.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ZFS uses a a data storage technique called "copy-on-write". This is covered more in the next section.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .17311 Tw (ZFS combines the ideas of a traditional filesystem with volmue and RAID management. You don't need to) Tj T* 0 Tw 2.373059 Tw (continue exporting blocks of data in layers like traditional GNU/Linux filesystems. This simplifies the) Tj T* 0 Tw (administration by quite a bit, and the training for future storage administrators is easier to handle.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 585.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .536654 Tw (Due to the rise of SSDs in the production storage market, ZFS was designed to take advantage of them.) Tj T* 0 Tw .384431 Tw (ZFS supports the wear leveling algorithms of SSDs, and as such, is suitable to use SSDs for external log) Tj T* 0 Tw (devices, and/or read-only caches.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 531.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 4.76152 Tw (Transparent deduplication and compression can be enabled to get more out of your storage.) Tj T* 0 Tw 1.349269 Tw (Deduplication does require a deduplication table to be maintained in RAM. As such, it's recommended) Tj T* 0 Tw 1.36811 Tw (that you have sufficient amounts of RAM if using deduplication. If the deduplication table spills over to) Tj T* 0 Tw (platter disk, this can drastically slow the filesystem down. Deduplication is handled at the block level.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 489.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .534651 Tw (However, compression does not incure such a performance penalty. ZFS supports both the gzip and lzjb) Tj T* 0 Tw .67436 Tw (compression algorithms, with the latter being the default. With gzip, levels 1 through 9 are supported, as) Tj T* 0 Tw (are standard on Unix.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 447.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 2.499982 Tw (After Oracle purchased Sun Microsystems, and the code for ZFS went proprietary, Oracle released) Tj T* 0 Tw 2.127318 Tw (encryption support for ZFS. This is native AES 256-bit encryption, without the need for any external) Tj T* 0 Tw (utilities. ZFS will take advantage of hardware AES instruction chips, should they be on the CPU die.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 345.0236 cm
q
BT 1 0 0 1 0 86 Tm 1.486457 Tw 12 TL /F1 10 Tf 0 0 0 rg (The most compelling feature of ZFS is that it was designed with ultimate data integrity in mind. Every) Tj T* 0 Tw .477765 Tw (block of the filesystem is checksummed with SHA-256 by default. When a block is read, it is hashed with) Tj T* 0 Tw 3.11284 Tw (the SHA-256 algorithm, then compared against the checksum in the metadata. This does incur a) Tj T* 0 Tw 1.945542 Tw (performance penalty, but as you'll see later, this is less of a concern. Further, all writes are done in) Tj T* 0 Tw 2.387318 Tw (transactions, where the pointers, blocks and log are written in one simultaneous write. As such, an) Tj T* 0 Tw 2.022339 Tw /F3 10 Tf (fsck\(8\) ) Tj /F1 10 Tf (is not needed with ZFS. As such, ZFS detects, and can fix, silent data errors imposed by) Tj T* 0 Tw 3.511751 Tw (hardware are any step of the write. Due to the atomic nature of transaction, the write is either) Tj T* 0 Tw (all-or-nothing. So, at worst, you have old data. You never have corrupted data.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 303.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .480514 Tw (Lastly, ZFS was written from the ground up by Jeff Bonwick. It does not contain any legacy UFS or other) Tj T* 0 Tw 1.944269 Tw (filesystem code. It's modern, and behaves much more like a transactional relational database than a) Tj T* 0 Tw (filesystem. Many of the old ideas were challenged, rethought, and implemented in new ways.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 270.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Copy-on-write) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 258.0236 cm
Q
q
1 0 0 1 62.69291 258.0236 cm
Q
q
1 0 0 1 62.69291 246.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Copy of block created, pointers updated) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 240.0236 cm
Q
q
1 0 0 1 62.69291 228.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Atomic- all or nothing) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 222.0236 cm
Q
q
1 0 0 1 62.69291 210.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Transactions- pointers, blocks, logs updated simultanously) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 204.0236 cm
Q
q
1 0 0 1 62.69291 192.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Uses a Merkle \(hash\) tree) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 186.0236 cm
Q
q
1 0 0 1 62.69291 174.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (No need for ) Tj /F3 10 Tf (fsck\(8\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 168.0236 cm
Q
q
1 0 0 1 62.69291 156.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ZFS "scrubs" data) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 156.0236 cm
Q
q
1 0 0 1 62.69291 138.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2832) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 96.02362 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .731412 Tw (Copy-on-write is a standardized data storage technique where rather than modifying the blocks in place,) Tj T* 0 Tw 1.819985 Tw (you copy the block to another location on disk. This does incure a performance penalty through disk) Tj T* 0 Tw (fragmentation, but opens up a world of features that are more difficult to use.) Tj T* ET
Q
Q
 
endstream
endobj
% 'R102': class PDFStream 
102 0 obj
% page stream
<< /Length 8638 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 717.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .536488 Tw (Because you have a copy of the block, with the pointers updated to point to the new block instead of the) Tj T* 0 Tw 3.11561 Tw (old, you can have filesystem revisions. This allows you to "rollback" to a previous version of the) Tj T* 0 Tw 2.63784 Tw (filesystem. This is typically done through snapshots, which ZFS fully supports. You can then clone) Tj T* 0 Tw (filesystems, and create forks for whatever reason.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL 1.123672 Tw (ZFS uses transactions to commit the data to disk rather than a journal. Suppose a file is 100 blocks in) Tj T* 0 Tw .471163 Tw (size, and you nede to modify 10 of them. With typical journaling filesystems, you create a journal, update) Tj T* 0 Tw .761988 Tw (the inode, modify the first block, then close the journal. This is done for all 10 blocks. Thus, 40 writes to) Tj T* 0 Tw .938876 Tw (disk happen in this scenario. Instead, ZFS will pool all 10 blocks, as well as metadata and log updates,) Tj T* 0 Tw 2.039986 Tw (into a single simultaneous transactional write flush. As such, there is no need for a journal, and the) Tj T* 0 Tw 1.497984 Tw (chances of a power outage leading to data corruption is greatly minimized. Lastly, all transactions are) Tj T* 0 Tw (atomic in nature- you either get it all, or you get nothing. You never only get part.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 585.0236 cm
q
BT 1 0 0 1 0 26 Tm 1.727356 Tw 12 TL /F1 10 Tf 0 0 0 rg (As a result, there is no need for a ) Tj /F3 10 Tf (fsck\(8\) ) Tj /F1 10 Tf (utility, as there is no journal to verify data consistency.) Tj T* 0 Tw .63811 Tw (Instead, ZFS uses a scrubbing technique that verifies \(and corrects in the event of redundancy\) the data) Tj T* 0 Tw (integrity. It's recommended that you scrub your data weekly.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 552.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Installing ZFS on GNU/Linux) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 540.0236 cm
Q
q
1 0 0 1 62.69291 540.0236 cm
Q
q
1 0 0 1 62.69291 528.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://zfsonlinux.org) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 522.0236 cm
Q
q
1 0 0 1 62.69291 510.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Ubuntu PPA) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 504.0236 cm
Q
q
1 0 0 1 62.69291 492.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (DEB and RPM supported) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 486.0236 cm
Q
q
1 0 0 1 62.69291 474.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (wget http://ae7.st/s/zfs-deb) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 468.0236 cm
Q
q
1 0 0 1 62.69291 456.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (dpkg -i zfsonlinux_1~wheezy_all.deb) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 450.0236 cm
Q
q
1 0 0 1 62.69291 438.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (apt-get update) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 432.0236 cm
Q
q
1 0 0 1 62.69291 420.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (apt-get install debian-zfs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 420.0236 cm
Q
q
1 0 0 1 62.69291 402.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2357) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 369.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (ZFS Commands) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 357.0236 cm
Q
q
1 0 0 1 62.69291 357.0236 cm
Q
q
1 0 0 1 62.69291 345.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (3 commands, one of which you will never use) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
Q
q
1 0 0 1 62.69291 327.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (zpool) Tj /F1 10 Tf (- Configures storage pools) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 321.0236 cm
Q
q
1 0 0 1 62.69291 309.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (zfs) Tj /F1 10 Tf (- Configures filesystems) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 303.0236 cm
Q
q
1 0 0 1 62.69291 291.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (zdb) Tj /F1 10 Tf (- Display pool debugging and consistency information) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 291.0236 cm
Q
q
1 0 0 1 62.69291 261.0236 cm
q
BT 1 0 0 1 0 14 Tm .497984 Tw 12 TL /F1 10 Tf 0 0 0 rg (Unlike traditional storage administration with GNU/Linux, there are only 3 commands that you need to be) Tj T* 0 Tw (familiar with, the latter of which you will never use: ) Tj /F3 10 Tf (zpool\(8\)) Tj /F1 10 Tf (, ) Tj /F3 10 Tf (zfs\(8\) ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (zdb\(8\)) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 207.0236 cm
q
BT 1 0 0 1 0 38 Tm 1.54248 Tw 12 TL /F1 10 Tf 0 0 0 rg (The ) Tj /F3 10 Tf (zpool\(8\) ) Tj /F1 10 Tf (command is responsible for creating, configuring, displaying and destroying your ZFS) Tj T* 0 Tw .658443 Tw (storage pools. This including adding drives, configuring hot spares, setting properties, such as advanced) Tj T* 0 Tw .536654 Tw (format drive detection, and a number of other things. If you need to do anything with your physical disks,) Tj T* 0 Tw (the ) Tj /F3 10 Tf (zpool\(8\) ) Tj /F1 10 Tf (is the command you reach for.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 141.0236 cm
q
BT 1 0 0 1 0 50 Tm 2.54248 Tw 12 TL /F1 10 Tf 0 0 0 rg (The ) Tj /F3 10 Tf (zfs\(8\) ) Tj /F1 10 Tf (command is responsible for creating, configuring, displaying and destroying your ZFS) Tj T* 0 Tw .29832 Tw (datasets. What you would typically call a "filelystem", ZFS refers to as a dataset. The reason being, is the) Tj T* 0 Tw 1.037356 Tw (filesystem stack is everything from your disks up to your data, of which a dataset is part of. Things like) Tj T* 0 Tw 1.12784 Tw (setting compression, sharing the dataset via NFS, SMB or iSCSI, creating snapshots, and sending and) Tj T* 0 Tw (receiving datasets over the wire.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 87.02362 cm
q
BT 1 0 0 1 0 38 Tm .482485 Tw 12 TL /F1 10 Tf 0 0 0 rg (The ) Tj /F3 10 Tf (zdb\(8\) ) Tj /F1 10 Tf (command is a debugging utility for ZFS that can display and configure your ZFS filesystem ) Tj T* 0 Tw .114983 Tw (by tuning various parameters. WARNING: There be dragons ahead! The ) Tj /F3 10 Tf (zdb\(8\) ) Tj /F1 10 Tf (command is a powerful ) Tj T* 0 Tw 1.299461 Tw (utility that allows you get get into the bowels of ZFS and make some very majoyr changes to how the ) Tj T* 0 Tw .679987 Tw (filesystem operates. You can completely screw up your data if you are not careful. It's likely that you will) Tj T* 0 Tw ET
Q
Q
 
endstream
endobj
% 'R103': class PDFStream 
103 0 obj
% page stream
<< /Length 7690 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (never need to use this command.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 720.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Beginner pool) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 708.0236 cm
Q
q
1 0 0 1 62.69291 708.0236 cm
Q
q
1 0 0 1 62.69291 696.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank sda sdb sdc) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 690.0236 cm
Q
q
1 0 0 1 62.69291 678.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool status tank) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 678.0236 cm
Q
q
1 0 0 1 62.69291 612.0236 cm
q
BT 1 0 0 1 0 50 Tm .930574 Tw 12 TL /F1 10 Tf 0 0 0 rg (To start off, let's create a simple pool. In this example, we have four disks: ) Tj /F3 10 Tf (/dev/sda) Tj /F1 10 Tf (, ) Tj /F3 10 Tf (/dev/sdb ) Tj /F1 10 Tf (and) Tj T* 0 Tw .517488 Tw /F3 10 Tf (/dev/sdc ) Tj /F1 10 Tf (that we wish to put into our storage pool. We'll call the storage pool ) Tj /F3 10 Tf (tank ) Tj /F1 10 Tf (for lack of a better) Tj T* 0 Tw .02832 Tw (word. Notice, that I don't have to put the absolute path to the block devices. Any block device discoverable) Tj T* 0 Tw .26528 Tw (under ) Tj /F3 10 Tf (/dev/ ) Tj /F1 10 Tf (can be referenced by its file name when using disks. Thus, referring to ) Tj /F3 10 Tf (/dev/sda as ) Tj /F1 10 Tf (just) Tj T* 0 Tw /F3 10 Tf (sda ) Tj /F1 10 Tf (is perfectly acceptable.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 582.0236 cm
q
BT 1 0 0 1 0 14 Tm .93936 Tw 12 TL /F1 10 Tf 0 0 0 rg (After creating our storage pool, you can see the status with the ) Tj /F3 10 Tf (zpool status ) Tj /F1 10 Tf (command. Our output) Tj T* 0 Tw (would look something like this:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 440.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 132 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F3 10 Tf 12 TL (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* (config:) Tj T*  T* (        NAME   STATE     READ WRITE CKSUM) Tj T* (         tank  ONLINE       0     0     0) Tj T* (          sda  ONLINE       0     0     0) Tj T* (          sdb  ONLINE       0     0     0) Tj T* (          sdc  ONLINE       0     0     0) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 407.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (VDEVs) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 395.8236 cm
Q
q
1 0 0 1 62.69291 395.8236 cm
Q
q
1 0 0 1 62.69291 383.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Virtual device managed by ZFS) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 377.8236 cm
Q
q
1 0 0 1 62.69291 365.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Always striped) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 359.8236 cm
Q
q
1 0 0 1 62.69291 347.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (disk \(default\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 341.8236 cm
Q
q
1 0 0 1 62.69291 329.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (file) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 323.8236 cm
Q
q
1 0 0 1 62.69291 311.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (mirror) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 305.8236 cm
Q
q
1 0 0 1 62.69291 293.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (raidz1/2/3) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 287.8236 cm
Q
q
1 0 0 1 62.69291 275.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (spare) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 269.8236 cm
Q
q
1 0 0 1 62.69291 257.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (log) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 251.8236 cm
Q
q
1 0 0 1 62.69291 239.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (cache) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 239.8236 cm
Q
q
1 0 0 1 62.69291 221.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2584) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 179.8236 cm
q
BT 1 0 0 1 0 26 Tm .22561 Tw 12 TL /F1 10 Tf 0 0 0 rg (VDEVs are virtual devices that are managed internally by ZFS. Similar to how Linux software RAID keeps) Tj T* 0 Tw 1.15561 Tw (a virtual block device ) Tj /F3 10 Tf (/dev/md0 ) Tj /F1 10 Tf (to represent multiple disks, ZFS does the same, although it does not) Tj T* 0 Tw (export a block device. There are seven VDEVs in ZFS, and VDEVs are always striped across each other.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 146.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Disk VDEV) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 134.8236 cm
Q
q
1 0 0 1 62.69291 134.8236 cm
Q
q
1 0 0 1 62.69291 122.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Dynamic striping \(RAID 0\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 116.8236 cm
Q
q
1 0 0 1 62.69291 104.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Disks can be full path, or those listed in ) Tj /F3 10 Tf (/dev/) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 98.82362 cm
Q
q
1 0 0 1 62.69291 86.82362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Best practice to use the disks in ) Tj /F3 10 Tf (/dev/disk/by-id/) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 80.82362 cm
Q
 
endstream
endobj
% 'R104': class PDFStream 
104 0 obj
% page stream
<< /Length 6909 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank sda sdb sdc) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 753.0236 cm
Q
q
1 0 0 1 62.69291 711.0236 cm
q
BT 1 0 0 1 0 26 Tm .969987 Tw 12 TL /F1 10 Tf 0 0 0 rg (This is the default VDEV, and can be any block device discoverable in ) Tj /F3 10 Tf (/dev/) Tj /F1 10 Tf (. You can provide the full) Tj T* 0 Tw 2.50528 Tw (path, such as ) Tj /F3 10 Tf (/dev/sda) Tj /F1 10 Tf (, or you can provide just the file ) Tj /F3 10 Tf (sda) Tj /F1 10 Tf (. Because each disk provided as an) Tj T* 0 Tw (argument is an individual VDEV, the data will be striped across all disks. An example would be:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 533.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 168 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 146 Tm /F3 10 Tf 12 TL (# zpool create tank sda sdb sdc sdd) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* (config:) Tj T*  T* (        NAME   STATE     READ WRITE CKSUM) Tj T* (         tank  ONLINE       0     0     0) Tj T* (          sda  ONLINE       0     0     0) Tj T* (          sdb  ONLINE       0     0     0) Tj T* (          sdc  ONLINE       0     0     0) Tj T* (          sdd  ONLINE       0     0     0) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 465.8236 cm
q
BT 1 0 0 1 0 50 Tm .759318 Tw 12 TL /F1 10 Tf 0 0 0 rg (A best practice would be to use the block devices found in ) Tj /F3 10 Tf (/dev/disk/by-id/ ) Tj /F1 10 Tf (instead of ) Tj /F3 10 Tf (/dev/sda) Tj /F1 10 Tf (,) Tj T* 0 Tw .413876 Tw (etc. The reason being, is some motherboards do not present the drives in the same order to Linux kernel) Tj T* 0 Tw 1.56561 Tw (on every boot. A drive discovered as ) Tj /F3 10 Tf (/dev/sda ) Tj /F1 10 Tf (on one boot might be discovered as ) Tj /F3 10 Tf (/dev/sdb ) Tj /F1 10 Tf (on) Tj T* 0 Tw 1.14122 Tw (another boot, whereas the ) Tj /F3 10 Tf (/dev/disk/by-id/ata-Maxtor_7L300S0_L60HZ9MH ) Tj /F1 10 Tf (will always be the) Tj T* 0 Tw (same, even though its symlink may be pointing to the newly assigned drive letter.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 411.8236 cm
q
BT 1 0 0 1 0 38 Tm .914651 Tw 12 TL /F1 10 Tf 0 0 0 rg (For the main storage pool, using devices ) Tj /F3 10 Tf (/dev/sda ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (/dev/sdb ) Tj /F1 10 Tf (generally isn't a problem, as ZFS) Tj T* 0 Tw .53683 Tw (can read the metadata on the disks, and rebuild the pool as necessary. As such, the reason for this best) Tj T* 0 Tw .355542 Tw (practice advice may not be clear now, but will become more clear when we discuss the ) Tj /F3 10 Tf (log ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (cache) Tj T* 0 Tw /F1 10 Tf (VDEVs.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 378.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (File VDEV) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 366.8236 cm
Q
q
1 0 0 1 62.69291 366.8236 cm
Q
q
1 0 0 1 62.69291 354.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Really only useful for testing configurations) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 348.8236 cm
Q
q
1 0 0 1 62.69291 336.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Requires the absolute path) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 330.8236 cm
Q
q
1 0 0 1 62.69291 318.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Must be preallocated) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 312.8236 cm
Q
q
1 0 0 1 62.69291 300.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Compression, encryption or deduplication) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 294.8236 cm
Q
q
1 0 0 1 62.69291 282.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Resizing pool) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 276.8236 cm
Q
q
1 0 0 1 62.69291 264.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Sandbox) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 258.8236 cm
Q
q
1 0 0 1 62.69291 246.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (NOT RECOMMENDED FOR PRODUCTION!!!) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 240.8236 cm
Q
q
1 0 0 1 62.69291 228.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank /tmp/file1 /tmp/file2 /tmp/file3) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 228.8236 cm
Q
q
1 0 0 1 62.69291 174.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.526098 Tw (Preallocated files can be used as storage for a pool. Sparse files will not work. You must provide the) Tj T* 0 Tw .876098 Tw (absolute path to every file when creating the pool. File VDEVs are not to be used for storing production) Tj T* 0 Tw .310651 Tw (data, but rather for testing scripts, setting properties, and understanding the internals of ZFS. An example) Tj T* 0 Tw (would be:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 81.62362 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 84 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F3 10 Tf 12 TL (# for i in {1..4}; do fallocate -l 1G /tmp/file$i; done) Tj T* (# zpool create tank /tmp/file1 /tmp/file2 /tmp/file3 /tmp/file4) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* ET
Q
Q
Q
Q
Q
 
endstream
endobj
% 'R105': class PDFStream 
105 0 obj
% page stream
<< /Length 4559 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 631.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 132 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F3 10 Tf 12 TL (config:) Tj T*  T* (        NAME          STATE     READ WRITE CKSUM) Tj T* (         tank         ONLINE       0     0     0) Tj T* (          /tmp/file1  ONLINE       0     0     0) Tj T* (          /tmp/file2  ONLINE       0     0     0) Tj T* (          /tmp/file3  ONLINE       0     0     0) Tj T* (          /tmp/file4  ONLINE       0     0     0) Tj T*  T* (errors: No known data errors) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 587.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.556654 Tw (When working with files in your storage pool, you should not create them from files that belong to an) Tj T* 0 Tw 1.194985 Tw (existing pool. Race conditions will occur, and you could end up with corrupted data. Rather, create the) Tj T* 0 Tw (files on another filesystem, such as ext4.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 554.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Mirror VDEV) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 542.8236 cm
Q
q
1 0 0 1 62.69291 542.8236 cm
Q
q
1 0 0 1 62.69291 530.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Standard RAID 1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 524.8236 cm
Q
q
1 0 0 1 62.69291 512.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Fastest redundancy for reads and writes) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 506.8236 cm
Q
q
1 0 0 1 62.69291 494.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Two or more disks) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 488.8236 cm
Q
q
1 0 0 1 62.69291 476.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank mirror sda sdb sdc sdd) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 476.8236 cm
Q
q
1 0 0 1 62.69291 434.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .964692 Tw (The standard RAID-1 mirror. All disks contain the same data as every other disk in the mirrored VDEV.) Tj T* 0 Tw .13104 Tw (This is the most expensive VDEV in terms of storage, but in terms of sequential reads and writes, will also) Tj T* 0 Tw (be the fastest. An example would be:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 221.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 204 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 182 Tm /F3 10 Tf 12 TL (# zpool create tank mirror sda sdb sdc sdd) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* (config:) Tj T*  T* (    NAME        STATE     READ WRITE CKSUM) Tj T* (    tank        ONLINE       0     0     0) Tj T* (      mirror-0  ONLINE       0     0     0) Tj T* (        sda     ONLINE       0     0     0) Tj T* (        sdb     ONLINE       0     0     0) Tj T* (        sdc     ONLINE       0     0     0) Tj T* (        sdd     ONLINE       0     0     0) Tj T*  T* (errors: No known data errors) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 189.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.395984 Tw (As mentioned previously, VDEVs are striped across each other. This makes it easy to create "nested") Tj T* 0 Tw (VDEVs. In this example, instead of mirroring 4 disks, would could create two mirror VDEV:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 84.42362 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 96 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F3 10 Tf 12 TL (# zpool create tank mirror sda sdb mirror sdc sdd) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* (config:) Tj T*  T* ET
Q
Q
Q
Q
Q
 
endstream
endobj
% 'R106': class PDFStream 
106 0 obj
% page stream
<< /Length 7777 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 631.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 132 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F3 10 Tf 12 TL (    NAME        STATE     READ WRITE CKSUM) Tj T* (    tank        ONLINE       0     0     0) Tj T* (      mirror-0  ONLINE       0     0     0) Tj T* (        sda     ONLINE       0     0     0) Tj T* (        sdb     ONLINE       0     0     0) Tj T* (      mirror-1  ONLINE       0     0     0) Tj T* (        sdc     ONLINE       0     0     0) Tj T* (        sdd     ONLINE       0     0     0) Tj T*  T* (errors: No known data errors) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 551.8236 cm
q
BT 1 0 0 1 0 62 Tm 1.356098 Tw 12 TL /F1 10 Tf 0 0 0 rg (Notice that the first mirrored VDEV is labeled as ) Tj /F3 10 Tf (mirror-0 ) Tj /F1 10 Tf (and is assigned to ) Tj /F3 10 Tf (sda ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (sdb ) Tj /F1 10 Tf (as we) Tj T* 0 Tw .094692 Tw (provided in our command. The second mirror VDEV is labeled as ) Tj /F3 10 Tf (mirror-1) Tj /F1 10 Tf (, and is assigned to ) Tj /F3 10 Tf (sdc ) Tj /F1 10 Tf (and) Tj T* 0 Tw .394692 Tw /F3 10 Tf (sdd ) Tj /F1 10 Tf (as we also provided in our command. Because VDEVs are always striped, the data is striped across) Tj T* 0 Tw 1.74784 Tw (the ) Tj /F3 10 Tf (mirror-0 ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (mirror-1 ) Tj /F1 10 Tf (VDEVs, thus creating our RAID-1+0. Note, this is not ) Tj /F3 10 Tf (RAID-10) Tj /F1 10 Tf (. The) Tj T* 0 Tw .014724 Tw (Linux kernel has a non-standard RAID-10 level, where it is not clear where the mirrors exist. In ZFS, this is) Tj T* 0 Tw (using two standardized RAID levels- a stripe of mirrors. Thus, it is RAID-1+0.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 521.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.159982 Tw (This helps regain space back from your mirror, while also greatly improving sequential performance.) Tj T* 0 Tw (Nested VDEVs will always outperform non-nested VDEVs in terms of sequential reads and writes.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 503.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (You can create nested VDEVs with mirrors or RAIDZ, as we'll discover here.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 470.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (RAID-Z VDEVs) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 458.8236 cm
Q
q
1 0 0 1 62.69291 458.8236 cm
Q
q
1 0 0 1 62.69291 446.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (RAID-Z1- Single distributed parity \(RAID 5 \(slow\)\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 440.8236 cm
Q
q
1 0 0 1 62.69291 428.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (RAID-Z2- Dual distributed parity \(RAID 6 \(slower\)\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 422.8236 cm
Q
q
1 0 0 1 62.69291 410.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (RAID-Z3- Triple distributed parity \(slowest\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 404.8236 cm
Q
q
1 0 0 1 62.69291 392.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank raidz1 sda sdb sdc) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 386.8236 cm
Q
q
1 0 0 1 62.69291 374.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank raidz2 sda sdb sdc sdd) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 368.8236 cm
Q
q
1 0 0 1 62.69291 356.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool create tank raidz3 sda sdb sdc sdd sde) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 356.8236 cm
Q
q
1 0 0 1 62.69291 338.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2590) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 260.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL 2.199982 Tw (To understand RAIDZ, you must first understand standards-based parity RAID, such as RAID-5 and) Tj T* 0 Tw .231654 Tw (RAID-6. With RAID-5, a single parity bit is distributed across the disks in a striped array, rather than using) Tj T* 0 Tw 1.388221 Tw (a dedicated parity disk, such as you would find in RAID-4. This balances reads, writes and disk wear.) Tj T* 0 Tw .956488 Tw (Further, it allows you to lose any one disk in the array, as the missing data can be re-calculated by the) Tj T* 0 Tw .444987 Tw (remaining data in the stripe, and the parity bit. RAID-6 uses a dual-distributed parity, and as a result, can) Tj T* 0 Tw (lose any two disks in the array before there is data loss.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 182.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL 1.003876 Tw (The problem with RAID-5 and RAID-6 is two-fold. First, the stripe is written to disk before the parity bit.) Tj T* 0 Tw .094488 Tw (This means that if there is a power outage after writing the stripe, and before writing the parity, you have a) Tj T* 0 Tw 1.267045 Tw (problem. This problem is referred to in the ZFS community as the "RAID-5 write hole". When power is) Tj T* 0 Tw 1.385251 Tw (restored, even though the data in the stripe might be consistent, the parity may not be the write bit to) Tj T* 0 Tw .440444 Tw (represent the XOR in the stripe. As such, if a disk failure occurs in the array, the parity will not be able to) Tj T* 0 Tw (help reconstruct the data, and you have bad data being sent to the application.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 140.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .988735 Tw (The second problem is the stripe width. In standards-based parity arrays, the stripe width is exactly n-1) Tj T* 0 Tw .043672 Tw (disks in the array if using RAID-5, and n-2 disks if using RAID-6. To determine your exact stripe width, use) Tj T* 0 Tw (the following calculation:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 134.8236 cm
Q
q
1 0 0 1 62.69291 134.8236 cm
Q
q
1 0 0 1 62.69291 122.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Divide the chunk size by the block size for one spindle/drive only. This gives you your stride size.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 116.8236 cm
Q
q
1 0 0 1 62.69291 92.82362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .433876 Tw (Then you take the stride size, and multiply it by the number of data-bearing disks in the RAID array.) Tj T* 0 Tw (This gives you the stripe width to use when formatting the volume.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 92.82362 cm
Q
 
endstream
endobj
% 'R107': class PDFStream 
107 0 obj
% page stream
<< /Length 8455 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 729.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.359461 Tw (If your chunk size is 512 KB \(default for Linux software RAID\), and your filesystem block size is 4 KB) Tj T* 0 Tw 1.176655 Tw (\(default for ext4\), then your stride size is 128 KB. If this is a 4-disk RAID-5, then your stripe width is a) Tj T* 0 Tw (static 384 KB.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 663.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 1.300898 Tw (Not all data you commit to disk will be in multiples of 384 KB. Most likely, you will spend a lot of time) Tj T* 0 Tw .081729 Tw (writing data that is less than 384 KB and you will spend a lot of time writing data is that more than 384 KB.) Tj T* 0 Tw .484488 Tw (So, if you commit data to a RAID-5 array that is only a few kilobytes, then when calculating the parity bit,) Tj T* 0 Tw .379989 Tw (you must XOR data on disks in the stripe that does not contain live, actual data. As a result, you spend a) Tj T* 0 Tw (great deal of time calculating the parity for "dead data".) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 645.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Now, let's take a look at ZFS RAIDZ.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 612.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Other VDEVs) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 588.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Spare- Hot spare to replace failed disks) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 582.0236 cm
Q
q
1 0 0 1 62.69291 570.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Log- Write-intensive separate LOG \(SLOG\) called ZFS Intent Log \(ZIL\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 564.0236 cm
Q
q
1 0 0 1 62.69291 552.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Cache- Read-intensive L2ARC cache) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 546.0236 cm
Q
q
1 0 0 1 62.69291 534.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Log and cache should and be on fast SSDs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 528.0236 cm
Q
q
1 0 0 1 62.69291 516.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool add tank spare sda) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 510.0236 cm
Q
q
1 0 0 1 62.69291 498.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool add tank log mirror sda sdb) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 492.0236 cm
Q
q
1 0 0 1 62.69291 480.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool add tank cache sdb sdc) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 480.0236 cm
Q
q
1 0 0 1 62.69291 426.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .672846 Tw (The spare VDEV allows us to create hot spares in the event of a disk failure. This will ensure that in the) Tj T* 0 Tw 2.314431 Tw (event of drive failure, a new disk will replace the failed disk, and you won't have a degraded pool.) Tj T* 0 Tw .382209 Tw (However, even though you can create a spare VDEV, it does not replace failed disk by default. You must) Tj T* 0 Tw (enable this after setting up your pool. This is done with the following command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 392.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zpool set autoreplace=on tank) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 348.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 3.391412 Tw (Two additional VDEVs allow us to create hybrid ZFS Storage pools. These VDEVs enhance the) Tj T* 0 Tw 1.09186 Tw (functionality of your pool, and can greatly decrease latencies, and response times if fast SSDs or RAM) Tj T* 0 Tw (drives are used. These are explained in the next slide.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 315.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (The Separate Intent Log \(SLOG\)) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 303.8236 cm
Q
q
1 0 0 1 62.69291 303.8236 cm
Q
q
1 0 0 1 62.69291 291.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (SSD, NVRAM or 15k RPM SAS) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 285.8236 cm
Q
q
1 0 0 1 62.69291 273.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Stores the ZFS Intent Log \(ZIL\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 267.8236 cm
Q
q
1 0 0 1 62.69291 255.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Write intensive) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 249.8236 cm
Q
q
1 0 0 1 62.69291 237.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Consider life expectency) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 231.8236 cm
Q
q
1 0 0 1 62.69291 219.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Can be very small) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 213.8236 cm
Q
q
1 0 0 1 62.69291 201.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Should be mirrored) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 201.8236 cm
Q
q
1 0 0 1 62.69291 183.8236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (http://pthree.org/?p=2564 ) Tj 0 0 0 rg (and ) Tj 0 0 .501961 rg (http://pthree.org/?p=2592) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 93.82362 cm
q
BT 1 0 0 1 0 74 Tm .528988 Tw 12 TL /F1 10 Tf 0 0 0 rg (The ) Tj /F3 10 Tf (log ) Tj /F1 10 Tf (VDEV should be setup with a fast SSD or RAM drive. It's known as a "separate log device", or) Tj T* 0 Tw 1.606303 Tw ("slog" for short. The ZFS intent log, or "ZIL" is typically stored on the same drives as your ZFS pool.) Tj T* 0 Tw .539513 Tw (However, if you add a SLOG to the pool, then the ZIL will be transferred to the SLOG. The ZIL acts very) Tj T* 0 Tw .927209 Tw (much like a journal, but it's not a journal. All synchronous write transactions will be written to the SLOG) Tj T* 0 Tw .290542 Tw (first, then flushed to slower platter later. ZFS flushes the SLOG to platter disk every 5 seconds by default.) Tj T* 0 Tw 1.311984 Tw (The SLOG is not used with asynchronous transactions, and not all synchronous writes use the SLOG.) Tj T* 0 Tw (Basically, only those transactions thas call O_SYNC or F_SYNC.) Tj T* ET
Q
Q
 
endstream
endobj
% 'R108': class PDFStream 
108 0 obj
% page stream
<< /Length 7282 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 705.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .308735 Tw (As an entertaining sidenote, the "SLOG" device is typically referred to as the "slogzilla". Your SLOG likely) Tj T* 0 Tw .914488 Tw (does not need to be very big. Of course, this all depends on your data needs, but in practice, it's pretty) Tj T* 0 Tw 3.215868 Tw (difficult to get the SLOG to see more than 1GB of data. Further, your SLOG needs to maintain) Tj T* 0 Tw .277045 Tw (consistency when no power is present. AN SSD is good for this, but battery-backed RAM drives can work) Tj T* 0 Tw (as well. The SLOG should be mirrored to maintain integrity.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 672.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (The Adjustable Replacement Cache) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 660.0236 cm
Q
q
1 0 0 1 62.69291 660.0236 cm
Q
q
1 0 0 1 62.69291 648.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Traditional Linux cache: MRU/LRU) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 642.0236 cm
Q
q
1 0 0 1 62.69291 630.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ZFS ARC is a modification of IBM ARC) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 624.0236 cm
Q
q
1 0 0 1 62.69291 612.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ARC in main RAM) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 606.0236 cm
Q
q
1 0 0 1 62.69291 594.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ARC stores an MRU and MFU pages) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 588.0236 cm
Q
q
1 0 0 1 62.69291 576.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Level 2 ARC \(L2ARC\) exists on fast disk) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 570.0236 cm
Q
q
1 0 0 1 62.69291 558.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (L2ARC stores ghost MRU and MFU pages) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 552.0236 cm
Q
q
1 0 0 1 62.69291 540.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Read intensive) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 534.0236 cm
Q
q
1 0 0 1 62.69291 522.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Should be very large) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 516.0236 cm
Q
q
1 0 0 1 62.69291 504.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Can be volatile and striped) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 504.0236 cm
Q
q
1 0 0 1 62.69291 486.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2659) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 420.0236 cm
q
BT 1 0 0 1 0 50 Tm 1.54104 Tw 12 TL /F1 10 Tf 0 0 0 rg (The ) Tj /F3 10 Tf (cache ) Tj /F1 10 Tf (VDEV should also be setup with a fast SSD or RAM drive. The cache is actually a level) Tj T* 0 Tw .061235 Tw (adjustable replacement cache, or "ARC". The ARC is stored in main system memory. The ARC stores two) Tj T* 0 Tw 1.193735 Tw (caches- the most recently used pages, and the most frequently used pages. When these pages fill the) Tj T* 0 Tw .917209 Tw (ARC, any additional request will push pages to a level two ARC, or "L2ARC". This is your cache VDEV) Tj T* 0 Tw (specified when building the pool.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 342.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL 1.19686 Tw (The L2ARC is a read-only cache. Active written data is not stored in the L2ARC. Actively synchronous) Tj T* 0 Tw .158409 Tw (written data is stored only in the SLOG and main platter drives. Because the L2ARC is a fast device, such) Tj T* 0 Tw 1.944692 Tw (as an SSD or RAM drive, this maintains that discarded MRU and MFU pages remain snappy to the) Tj T* 0 Tw .207633 Tw (application. The L2ARC should be as large as possible, and to increase throughput, can be striped. In the) Tj T* 0 Tw 1.54881 Tw (event of a power outage, the L2ARC data will be discarded, which is okay, because the data already) Tj T* 0 Tw (exists on slower platter disk, and can be rebuilt when power resumes.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 309.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Hybrid Storage Pools) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 297.0236 cm
Q
q
1 0 0 1 62.69291 297.0236 cm
Q
q
1 0 0 1 62.69291 273.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F3 10 Tf 12 TL .683828 Tw (zpool create tank mirror sda sdb mirror sdc sdd log mirror sde1 sf1 cache) Tj T* 0 Tw (sde2 sdf2 spare sdg) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 273.0236 cm
Q
q
1 0 0 1 62.69291 231.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.741654 Tw (To illustrate setting up a hybrid storage pool, suppose we have two 40 GB Intel SSDs. Each SSD is) Tj T* 0 Tw .260988 Tw (partitioned the same, where the first partition is 1 GB in size for the SLOG, and the second partition is the) Tj T* 0 Tw (remaining 39 GB for the L2ARC. Here is how you could set it up, with 4 disks in a RAID-1+0:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 82.98258 cm
q
q
.96447 0 0 .96447 0 0 cm
q
1 0 0 1 6.6 6.843137 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 486 144 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 122 Tm /F3 10 Tf 12 TL (# zpool create tank mirror sda sdb mirror sdc sdd log sde1 sdf1 cache sde2 sdf2) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* ( scan: none requested) Tj T* (config:) Tj T*  T* (    NAME        STATE     READ WRITE CKSUM) Tj T* (    tank        ONLINE       0     0     0) Tj T* (      mirror-0  ONLINE       0     0     0) Tj T* (        sda     ONLINE       0     0     0) Tj T* ET
Q
Q
Q
Q
Q
 
endstream
endobj
% 'R109': class PDFStream 
109 0 obj
% page stream
<< /Length 6556 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 595.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 168 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 146 Tm /F3 10 Tf 12 TL (        sdb     ONLINE       0     0     0) Tj T* (      mirror-1  ONLINE       0     0     0) Tj T* (        sdc     ONLINE       0     0     0) Tj T* (        sdd     ONLINE       0     0     0) Tj T* (    logs) Tj T* (      mirror-2  ONLINE       0     0     0) Tj T* (        sde1    ONLINE       0     0     0) Tj T* (        sdf1    ONLINE       0     0     0) Tj T* (    cache) Tj T* (      sde2      ONLINE       0     0     0) Tj T* (      sdf2      ONLINE       0     0     0) Tj T*  T* (errors: No known data errors) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 562.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Export and Import pools) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 550.8236 cm
Q
q
1 0 0 1 62.69291 550.8236 cm
Q
q
1 0 0 1 62.69291 538.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Offlines all disks) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 532.8236 cm
Q
q
1 0 0 1 62.69291 520.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Creates ) Tj /F3 10 Tf (/etc/zfs/zpool.cache) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 514.8236 cm
Q
q
1 0 0 1 62.69291 502.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Useful for migrating disks) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 502.8236 cm
Q
q
1 0 0 1 62.69291 484.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2594) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 442.8236 cm
q
BT 1 0 0 1 0 26 Tm .63528 Tw 12 TL /F1 10 Tf 0 0 0 rg (Exporting a ZFS pool allows you to migrate disks from one storage server to another. Exporting the pool) Tj T* 0 Tw .814431 Tw (will create a cache file found in /etc/zfs/zpool.cache, which can then be used on the new server, if need) Tj T* 0 Tw (be. This is done with the ) Tj /F3 10 Tf (zpool export ) Tj /F1 10 Tf (command.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 400.8236 cm
q
BT 1 0 0 1 0 26 Tm .281417 Tw 12 TL /F1 10 Tf 0 0 0 rg (Just as you can export a ZFS pool, you can use the ) Tj /F3 10 Tf (zfs import ) Tj /F1 10 Tf (command to import a pool on the new) Tj T* 0 Tw .323876 Tw (server. However, if the pool was not properly exported, then ZFS will complain, and will not let you import) Tj T* 0 Tw (the pool on the new server.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 346.8236 cm
q
BT 1 0 0 1 0 38 Tm .441894 Tw 12 TL /F1 10 Tf 0 0 0 rg (If you destroy a ZFS pool, you can still import the pool- not all is lost. Just used the ) Tj /F3 10 Tf (zpool import -D) Tj T* 0 Tw 1.032209 Tw /F1 10 Tf (switch with the command to import a destroyed pool. ZFS will then do a sanity check on the metadata,) Tj T* 0 Tw .275318 Tw (making sure everything looks good, before bringing the pool ONLINE. This may also mean scrubbing and) Tj T* 0 Tw (resilvering the data as needed.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 304.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.421163 Tw (WARNING: You should only import ZFS pools onto servers that have the same or a newer version of) Tj T* 0 Tw 2.323145 Tw (ZFS. ZFS will not allow you to import pools that have a newer version on the source, than on the) Tj T* 0 Tw (destination.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 271.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Scrub and Resilver pools) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 259.8236 cm
Q
q
1 0 0 1 62.69291 259.8236 cm
Q
q
1 0 0 1 62.69291 247.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Every block \(data and meta\) SHA-256 checksummed) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 241.8236 cm
Q
q
1 0 0 1 62.69291 229.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Self healing with redundancy) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 223.8236 cm
Q
q
1 0 0 1 62.69291 211.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool scrub tank) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 205.8236 cm
Q
q
1 0 0 1 62.69291 193.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Resilver rebuilds missing data on new disk) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 187.8236 cm
Q
q
1 0 0 1 62.69291 175.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool replace tank sde sdh) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 175.8236 cm
Q
q
1 0 0 1 62.69291 157.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2630) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 103.8236 cm
q
BT 1 0 0 1 0 38 Tm .891984 Tw 12 TL /F1 10 Tf 0 0 0 rg (Most filesystem utilities have a standardized way to do data validation through an ) Tj /F3 10 Tf (fsck\(8\) ) Tj /F1 10 Tf (utility. This) Tj T* 0 Tw .70936 Tw (utility is typically for journaled filesystems, and reads the journal, along with the inodes and metadata, to) Tj T* 0 Tw 2.879069 Tw (determine data consistency. Unfortunately, these utilities don't provide protection against silent disk) Tj T* 0 Tw (errors, nor can they be run while the disk or partition is online.) Tj T* ET
Q
Q
 
endstream
endobj
% 'R110': class PDFStream 
110 0 obj
% page stream
<< /Length 6049 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 717.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.844269 Tw (With ZFS, every block, both metadata and data, is checksummed using the SHA-256 algorithm. This) Tj T* 0 Tw 1.15881 Tw (means that if data corruption were to occur, it is practically impossible that the corrupted block has the) Tj T* 0 Tw 1.509985 Tw (same SHA-256 checksum as what it was originally intended to be. The checksum itself resides in the) Tj T* 0 Tw (matadata.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 651.0236 cm
q
BT 1 0 0 1 0 50 Tm .27528 Tw 12 TL /F1 10 Tf 0 0 0 rg (There are two ways in which ZFS knows that a block has been corrupted: through manual scrubbing, and) Tj T* 0 Tw .669269 Tw (through automatic detection by application requests. Scrubbing is similar in nature to a typical ) Tj /F3 10 Tf (fsck\(8\)) Tj /F1 10 Tf (,) Tj T* 0 Tw .477356 Tw (except that the scrub can happen whele the data is online. If corruption is detected in the scrub, then the) Tj T* 0 Tw .438221 Tw (block can be corrected if good data exists elsewhere in a redundant pool. It is recommended to manually) Tj T* 0 Tw (scrub your data once per week for consumer drives, and once per month for enterprise drives.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 573.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL .772619 Tw (As disks die, they need to be replaced. This means that when replacing a disk in ZFS, the data missing) Tj T* 0 Tw 1.802765 Tw (needs to be repopulated onto the new disk. However, in ZFS our rebuild of the data is smarter than) Tj T* 0 Tw .13104 Tw (standard utilities, such as MD RAID in the Linux kernel. Most rebuilds will read the entire disks in the pool,) Tj T* 0 Tw 1.714104 Tw (and make the necessary changes on the new disk. With ZFS, we know where live data resides, and) Tj T* 0 Tw .083903 Tw (where it doesn't. So, we don't need to read all the blocks on the all the disks, we only need to read the live) Tj T* 0 Tw (data. As such, ZFS refers to this as a resilver.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 555.0236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (To replace a disk in an array, use the ) Tj /F3 10 Tf (zfs replace ) Tj /F1 10 Tf (command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 269.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 276 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 254 Tm /F3 10 Tf 12 TL (# zpool replace tank sde sde) Tj T* (# zpool status tank) Tj T* (  pool: tank) Tj T* ( state: ONLINE) Tj T* (status: One or more devices is currently being resilvered.  The pool will) Tj T* (        continue to function, possibly in a degraded state.) Tj T* (action: Wait for the resilver to complete.) Tj T* ( scrub: resilver in progress for 0h2m, 16.43% done, 0h13m to go) Tj T* (config:) Tj T*  T* (        NAME          STATE       READ WRITE CKSUM) Tj T* (        tank          DEGRADED       0     0     0) Tj T* (          mirror-0    DEGRADED       0     0     0) Tj T* (            replacing DEGRADED       0     0     0) Tj T* (            sde       ONLINE         0     0     0) Tj T* (            sdf       ONLINE         0     0     0) Tj T* (          mirror-1    ONLINE         0     0     0) Tj T* (            sdg       ONLINE         0     0     0) Tj T* (            sdh       ONLINE         0     0     0) Tj T* (          mirror-2    ONLINE         0     0     0) Tj T* (            sdi       ONLINE         0     0     0) Tj T* (            sdj       ONLINE         0     0     0) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 236.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Zpool Properties) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 224.8236 cm
Q
q
1 0 0 1 62.69291 224.8236 cm
Q
q
1 0 0 1 62.69291 212.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zpool get all tank) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 206.8236 cm
Q
q
1 0 0 1 62.69291 194.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (ashift=9 ) Tj /F1 10 Tf (or ) Tj /F3 10 Tf (ashift=12) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 188.8236 cm
Q
q
1 0 0 1 62.69291 176.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (autoexpand ) Tj /F1 10 Tf (off by default) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 170.8236 cm
Q
q
1 0 0 1 62.69291 158.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (autoreplace ) Tj /F1 10 Tf (off by default) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 152.8236 cm
Q
q
1 0 0 1 62.69291 140.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (health ) Tj /F1 10 Tf (shows status of pool) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 140.8236 cm
Q
q
1 0 0 1 62.69291 122.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2632) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 80.82362 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .360514 Tw (The ZFS pool has a number of different properties that you can apply that affect the operation of the pool) Tj T* 0 Tw .341163 Tw (differently. Think of this as tuning parameters for the filesystem at the pool level. The properties are listed) Tj T* 0 Tw (as follows:) Tj T* ET
Q
Q
 
endstream
endobj
% 'R111': class PDFStream 
111 0 obj
% page stream
<< /Length 4397 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
q
BT 1 0 0 1 0 14 Tm .006988 Tw 12 TL /F1 10 Tf 0 0 0 rg (To get a property from the pool, use the ) Tj /F3 10 Tf (zpool get ) Tj /F1 10 Tf (command. You can provide the ) Tj /F3 10 Tf (all ) Tj /F1 10 Tf (option to list all) Tj T* 0 Tw (properties of the pool, or you can list properties that you want to view, comma-separated:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 323.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 408 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 386 Tm /F3 10 Tf 12 TL (# zpool get health tank) Tj T* (NAME  PROPERTY  VALUE   SOURCE) Tj T* (tank  health    ONLINE  -) Tj T*  T* (# zpool get health,free,allocated tank) Tj T* (NAME  PROPERTY   VALUE   SOURCE) Tj T* (tank  health     ONLINE  -) Tj T* (tank  free       176G    -) Tj T* (tank  allocated  32.2G   -) Tj T*  T* (# zpool get all tank) Tj T* (NAME  PROPERTY       VALUE       SOURCE) Tj T* (tank  size           208G        -) Tj T* (tank  capacity       15%         -) Tj T* (tank  altroot        -           default) Tj T* (tank  health         ONLINE      -) Tj T* (tank  guid           1695112377970346970  default) Tj T* (tank  version        28          default) Tj T* (tank  bootfs         -           default) Tj T* (tank  delegation     on          default) Tj T* (tank  autoreplace    off         default) Tj T* (tank  cachefile      -           default) Tj T* (tank  failmode       wait        default) Tj T* (tank  listsnapshots  off         default) Tj T* (tank  autoexpand     off         default) Tj T* (tank  dedupditto     0           default) Tj T* (tank  dedupratio     1.00x       -) Tj T* (tank  free           176G        -) Tj T* (tank  allocated      32.2G       -) Tj T* (tank  readonly       off         -) Tj T* (tank  ashift         0           default) Tj T* (tank  comment        -           default) Tj T* (tank  expandsize     0           -) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 291.8236 cm
q
BT 1 0 0 1 0 14 Tm 1.556303 Tw 12 TL /F1 10 Tf 0 0 0 rg (All of the properties can be found at my blog post above, in the ) Tj /F3 10 Tf (zpool\(8\) ) Tj /F1 10 Tf (manual, or at the official) Tj T* 0 Tw (Solaris documentation.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 249.8236 cm
q
BT 1 0 0 1 0 26 Tm .92832 Tw 12 TL /F1 10 Tf 0 0 0 rg (To set a property, use the ) Tj /F3 10 Tf (zfs set ) Tj /F1 10 Tf (command. However, there is a catch: for properties that require a) Tj T* 0 Tw .946303 Tw (string argument, there is no way to get back to default without know what the default is beforehand. If I) Tj T* 0 Tw (wished to set the comment property for the pool, I could do the following:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 180.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 60 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F3 10 Tf 12 TL (# zfs set comment="Contact admins@example.com" tank) Tj T* (# zpool get comment tank) Tj T* (NAME  PROPERTY  VALUE                       SOURCE) Tj T* (tank  comment   Contact admins@example.com  local) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 147.6236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Zpool Best Practices and Caveats) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 135.6236 cm
Q
q
1 0 0 1 62.69291 135.6236 cm
Q
q
1 0 0 1 62.69291 123.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (64-bit) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 117.6236 cm
Q
q
1 0 0 1 62.69291 105.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Gobs of ECC RAM) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 99.62362 cm
Q
q
1 0 0 1 62.69291 87.62362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Use whole disks) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 81.62362 cm
Q
 
endstream
endobj
% 'R112': class PDFStream 
112 0 obj
% page stream
<< /Length 10106 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Keep pools separate) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 747.0236 cm
Q
q
1 0 0 1 62.69291 735.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Speed: RAID-1+0 > RAIDZ1 > RAIDZ2 > RAIDZ3) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 729.0236 cm
Q
q
1 0 0 1 62.69291 717.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Power of 2 plus parity) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 711.0236 cm
Q
q
1 0 0 1 62.69291 699.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Fast disk for SLOG & L2ARC) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 693.0236 cm
Q
q
1 0 0 1 62.69291 681.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Regular scrubs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
Q
q
1 0 0 1 62.69291 663.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Enable compression) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 663.0236 cm
Q
q
1 0 0 1 62.69291 645.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2782) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 615.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .637209 Tw (As with most "best practices", these guidelines do carry a great amount of weight, but they also may not) Tj T* 0 Tw (work for your environment. Heed the advice, but do as you must:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 609.0236 cm
Q
q
1 0 0 1 62.69291 609.0236 cm
Q
q
1 0 0 1 62.69291 585.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.424985 Tw (Only run ZFS on 64-bit kernels. It has 64-bit specific code that 32-bit kernels cannot do anything) Tj T* 0 Tw (with.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 579.0236 cm
Q
q
1 0 0 1 62.69291 555.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .432262 Tw (Install ZFS only on a system with lots of RAM. 1 GB is a bare minimum, 2 GB is better, 4 GB would) Tj T* 0 Tw (be preferred to start. Remember, ZFS will use 7/8 of the available RAM for the ARC.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 525.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .439983 Tw (Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency. The) Tj T* 0 Tw (ARC is an actual read-only data cache of valuable data in RAM.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 519.0236 cm
Q
q
1 0 0 1 62.69291 483.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .557209 Tw (Use whole disks rather than partitions. ZFS can make better use of the on-disk cache as a result. If) Tj T* 0 Tw 1.703984 Tw (you must use partitions, backup the partition table, and take care when reinstalling data into the) Tj T* 0 Tw (other partitions, so you don't corrupt the data in your pool.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 477.0236 cm
Q
q
1 0 0 1 62.69291 453.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .938409 Tw (Keep each VDEV in a storage pool the same size. If VDEVs vary in size, ZFS will favor the larger) Tj T* 0 Tw (VDEV, which could lead to performance bottlenecks.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 447.0236 cm
Q
q
1 0 0 1 62.69291 411.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .06332 Tw (Use redundancy when possible, as ZFS can and will want to correct data errors that exist in the pool.) Tj T* 0 Tw .370542 Tw (You cannot fix these errors if you do not have a redundant good copy elsewhere in the pool. Mirrors) Tj T* 0 Tw (and RAID-Z levels accomplish this.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 405.0236 cm
Q
q
1 0 0 1 62.69291 345.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 45 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .107045 Tw (For the number of disks in the storage pool, use the "power of two plus parity" recommendation. This) Tj T* 0 Tw .287485 Tw (is for storage space efficiency and hitting the "sweet spot" in performance. So, for a RAIDZ-1 VDEV,) Tj T* 0 Tw .604431 Tw (use three \(2+1\), five \(4+1\), or nine \(8+1\) disks. For a RAIDZ-2 VDEV, use four \(2+2\), six \(4+2\), ten) Tj T* 0 Tw .54061 Tw (\(8+2\), or eighteen \(16+2\) disks. For a RAIDZ-3 VDEV, use five \(2+3\), seven \(4+3\), eleven \(8+3\), or) Tj T* 0 Tw (nineteen \(16+3\) disks. For pools larger than this, consider striping across mirrored VDEVs.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
Q
q
1 0 0 1 62.69291 267.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL 2.433555 Tw (Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1. You've heard the phrase "when it rains, it) Tj T* 0 Tw 2.447882 Tw (pours". This is true for disk failures. If a disk fails in a RAIDZ-1, and the hot spare is getting) Tj T* 0 Tw .696235 Tw (resilvered, until the data is fully copied, you cannot afford another disk failure during the resilver, or) Tj T* 0 Tw .906235 Tw (you will suffer data loss. With RAIDZ-2, you can suffer two disk failures, instead of one, increasing) Tj T* 0 Tw 1.000651 Tw (the probability you have fully resilvered the necessary data before the second, and even third disk) Tj T* 0 Tw (fails.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 261.0236 cm
Q
q
1 0 0 1 62.69291 225.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .313516 Tw (Perform regular \(at least weekly\) backups of the full storage pool. It's not a backup, unless you have) Tj T* 0 Tw 1.592651 Tw (multiple copies. Just because you have redundant disk, does not ensure live running data in the) Tj T* 0 Tw (event of a power failure, hardware failure or disconnected cables.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 219.0236 cm
Q
q
1 0 0 1 62.69291 195.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm .22561 Tw 12 TL /F1 10 Tf 0 0 0 rg (Use hot spares to quickly recover from a damaged device. Set the ) Tj /F3 10 Tf (autoreplace ) Tj /F1 10 Tf (property to on for) Tj T* 0 Tw (the pool.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 189.0236 cm
Q
q
1 0 0 1 62.69291 165.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.739985 Tw (Consider using a hybrid storage pool with fast SSDs or NVRAM drives. Using a fast SLOG and) Tj T* 0 Tw (L2ARC can greatly improve performance.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 159.0236 cm
Q
q
1 0 0 1 62.69291 147.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 141.0236 cm
Q
q
1 0 0 1 62.69291 105.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .720574 Tw (If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you) Tj T* 0 Tw .505703 Tw (will need it, 1 GB is likely sufficient for your SLOG. Use the rest of the SSD or NVRAM drive for the) Tj T* 0 Tw (L2ARC. The more storage for the L2ARC, the better.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 99.02362 cm
Q
 
endstream
endobj
% 'R113': class PDFStream 
113 0 obj
% page stream
<< /Length 9902 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.223318 Tw (Keep pool capacity under 80% for best performance. Due to the copy-on-write nature of ZFS, the) Tj T* 0 Tw (filesystem gets heavily fragmented.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 735.0236 cm
Q
q
1 0 0 1 62.69291 723.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Email reports of capacity at least monthly.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 717.0236 cm
Q
q
1 0 0 1 62.69291 693.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.690814 Tw (Scrub consumer-grade SATA and SCSI disks weekly and enterprise-grade SAS and FC disks) Tj T* 0 Tw (monthly.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 687.0236 cm
Q
q
1 0 0 1 62.69291 663.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 5.004597 Tw (Email reports of the storage pool health weekly for redundant arrays, and bi-weekly for) Tj T* 0 Tw (non-redundant arrays.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 657.0236 cm
Q
q
1 0 0 1 62.69291 633.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm .537633 Tw 12 TL /F1 10 Tf 0 0 0 rg (When using advanced format disks that read and write data in 4 KB sectors, set the ) Tj /F3 10 Tf (ashift ) Tj /F1 10 Tf (value) Tj T* 0 Tw (to 12 on pool creation for maximum performance. Default is 9 for 512-byte sectors.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
Q
q
1 0 0 1 62.69291 603.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm .334692 Tw 12 TL /F1 10 Tf 0 0 0 rg (Set ) Tj /F3 10 Tf (autoexpand ) Tj /F1 10 Tf (to on, so you can expand the storage pool automatically after all disks in the pool) Tj T* 0 Tw (have been replaced with larger ones. Default is off.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 597.0236 cm
Q
q
1 0 0 1 62.69291 585.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Always export your storage pool when moving the disks from one physical system to another.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 579.0236 cm
Q
q
1 0 0 1 62.69291 519.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 45 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 2.736342 Tw (When considering performance, know that for sequential writes, mirrors will always outperform) Tj T* 0 Tw .486412 Tw (RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mirrors on smaller) Tj T* 0 Tw .038735 Tw (data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem) Tj T* 0 Tw .365697 Tw (to perform in similar manners. Striped mirrors will outperform mirrors and RAID-Z in both sequential,) Tj T* 0 Tw (and random reads and writes.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 513.0236 cm
Q
q
1 0 0 1 62.69291 465.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.933059 Tw (Compression is disabled by default. This doesn't make much sense with today's hardware. ZFS) Tj T* 0 Tw 2.224983 Tw (compression is extremely cheap, extremely fast, and barely adds any latency to the reads and) Tj T* 0 Tw 1.949269 Tw (writes. In fact, in some scenarios, your disks will respond faster with compression enabled than) Tj T* 0 Tw (disabled. A further benefit is the massive space benefits.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 465.0236 cm
Q
q
1 0 0 1 62.69291 435.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .662619 Tw (As with any "best practices" list, there is also warnings and caveats you should be aware of. As with the) Tj T* 0 Tw (list above, this is by no means exhaustive, but following it should help you in most situations:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 429.0236 cm
Q
q
1 0 0 1 62.69291 429.0236 cm
Q
q
1 0 0 1 62.69291 405.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .001163 Tw (Your VDEVs determine the IOPS of the storage, and the slowest disk in that VDEV will determine the) Tj T* 0 Tw (IOPS for the entire VDEV.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 399.0236 cm
Q
q
1 0 0 1 62.69291 351.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm 1.389431 Tw 12 TL /F1 10 Tf 0 0 0 rg (ZFS uses 1/64 of the available raw storage for metadata. So, if you purchased a 1 TB drive, the) Tj T* 0 Tw 1.078935 Tw (actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The ) Tj /F3 10 Tf (zfs) Tj T* 0 Tw 1.234597 Tw (list ) Tj /F1 10 Tf (command will show an accurate representation of your available storage. Plan your storage) Tj T* 0 Tw (keeping this in mind.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 345.0236 cm
Q
q
1 0 0 1 62.69291 297.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.288735 Tw (ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks,) Tj T* 0 Tw .313735 Tw (self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure) Tj T* 0 Tw .287356 Tw (to configure it as a true JBOD \(or "passthrough mode"\), so ZFS can control the disks. If you can't do) Tj T* 0 Tw (this with your RAID card, don't use it. Best to use a real HBA.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 291.0236 cm
Q
q
1 0 0 1 62.69291 255.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .954983 Tw (Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure) Tj T* 0 Tw .16528 Tw (greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt,) Tj T* 0 Tw (mdadm or LVM beneath ZFS.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 249.0236 cm
Q
q
1 0 0 1 62.69291 201.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.599985 Tw (Do not share a SLOG or L2ARC DEVICE across pools. Each pool should have its own physical) Tj T* 0 Tw 1.082927 Tw (DEVICE, not logical drive, as is the case with some PCI-Express SSD cards. Use the full card for) Tj T* 0 Tw 1.765868 Tw (one pool, and a different physical card for another pool. If you share a physical device, you will) Tj T* 0 Tw (create race conditions, and could end up with corrupted data.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 195.0236 cm
Q
q
1 0 0 1 62.69291 159.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .906235 Tw (Do not share a single storage pool across different servers. ZFS is not a clustered filesystem. Use) Tj T* 0 Tw .587765 Tw (GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a) Tj T* 0 Tw (shared storage backend.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 153.0236 cm
Q
q
1 0 0 1 62.69291 81.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL .222488 Tw (Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one) Tj T* 0 Tw 1.36332 Tw (VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be) Tj T* 0 Tw 4.225984 Tw (RAIDZ-1. Unless of course, you know what you are doing, and are willing to accept the) Tj T* 0 Tw 2.024983 Tw (consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different) Tj T* 0 Tw .239983 Tw (redundancy can lead to performance issues and space efficiency concerns, and make it very difficult) Tj T* 0 Tw (to recover in the event of a failure.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 76.86614 cm
Q
 
endstream
endobj
% 'R114': class PDFStream 
114 0 obj
% page stream
<< /Length 10562 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.310574 Tw (Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent) Tj T* 0 Tw (mass drive failure.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 735.0236 cm
Q
q
1 0 0 1 62.69291 711.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.113488 Tw (In fact, do not mix disk sizes or speeds in your storage pool at all. Do not mix disk counts across) Tj T* 0 Tw (VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 681.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.668409 Tw (Do not put all the drives from a single controller in one VDEV. Plan your storage, such that if a) Tj T* 0 Tw (controller fails, it affects only the number of disks necessary to keep the data online.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
Q
q
1 0 0 1 62.69291 639.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .277209 Tw 12 TL /F1 10 Tf 0 0 0 rg (When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be) Tj T* 0 Tw 1.684269 Tw (changed after the fact. Use ) Tj /F3 10 Tf (zpool create -o ashift=12 tank mirror sda sdb ) Tj /F1 10 Tf (as an) Tj T* 0 Tw (example.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 633.0236 cm
Q
q
1 0 0 1 62.69291 597.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .14683 Tw 12 TL /F1 10 Tf 0 0 0 rg (Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable) Tj T* 0 Tw 1.116412 Tw (this feature. Set the autoreplace feature to on. Use ) Tj /F3 10 Tf (zpool set autoreplace=on tank ) Tj /F1 10 Tf (as an) Tj T* 0 Tw (example.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 591.0236 cm
Q
q
1 0 0 1 62.69291 555.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .372209 Tw 12 TL /F1 10 Tf 0 0 0 rg (The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by) Tj T* 0 Tw .696235 Tw (larger ones. You MUST enable this feature, and you MUST enable it before replacing the first disk.) Tj T* 0 Tw (Use ) Tj /F3 10 Tf (zpool set autoexpand=on tank ) Tj /F1 10 Tf (as an example.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 489.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 45 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 1.357485 Tw (ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new) Tj T* 0 Tw .342765 Tw (device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This) Tj T* 0 Tw .227717 Tw (will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such) Tj T* 0 Tw .525868 Tw (mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will) Tj T* 0 Tw (not rebuild the stripe width.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 483.0236 cm
Q
q
1 0 0 1 62.69291 471.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (You cannot shrink a pool, only grow it. This means you cannot remove VDEVs from a storage pool.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 465.0236 cm
Q
q
1 0 0 1 62.69291 441.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm 1.459269 Tw 12 TL /F1 10 Tf 0 0 0 rg (You can only remove drives from mirrored VDEV using the ) Tj /F3 10 Tf (zpool detach ) Tj /F1 10 Tf (command. You can) Tj T* 0 Tw (replace drives with another drive in RAIDZ and mirror VDEVs however.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 435.0236 cm
Q
q
1 0 0 1 62.69291 411.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .124987 Tw (Do not create a storage pool of files or ZVOLs from an existing pool. Race conditions will be present,) Tj T* 0 Tw (and you will end up with corrupted data. Always keep multiple pools separate.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 405.0236 cm
Q
q
1 0 0 1 62.69291 369.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .865542 Tw (The Linux kernel may not assign a drive the same drive letter at every boot. Thus, you should use) Tj T* 0 Tw .465318 Tw (the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your pool devices could end) Tj T* 0 Tw (up as a SLOG device, which would in turn clobber your ZFS data.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 363.0236 cm
Q
q
1 0 0 1 62.69291 339.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.89784 Tw (Don't create massive storage pools "just because you can". Even though ZFS can create 78-bit) Tj T* 0 Tw (storage pool sizes, that doesn't mean you need to create one.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 333.0236 cm
Q
q
1 0 0 1 62.69291 321.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Don't put production directly into the pool. Use ZFS datasets instead.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 315.0236 cm
Q
q
1 0 0 1 62.69291 291.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .92561 Tw (Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the) Tj T* 0 Tw (ins and outs of ZFS.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 291.0236 cm
Q
q
1 0 0 1 62.69291 258.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Creating ZFS Datasets) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 246.0236 cm
Q
q
1 0 0 1 62.69291 246.0236 cm
Q
q
1 0 0 1 62.69291 234.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Combined volume manager & filesystem) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 228.0236 cm
Q
q
1 0 0 1 62.69291 216.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Filesystem == Dataset) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 210.0236 cm
Q
q
1 0 0 1 62.69291 198.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Filesystems have all pool storage) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 192.0236 cm
Q
q
1 0 0 1 62.69291 180.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Filesystems aware of each other's storage) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 174.0236 cm
Q
q
1 0 0 1 62.69291 162.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Nested datasets possible) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 156.0236 cm
Q
q
1 0 0 1 62.69291 144.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs create tank/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 138.0236 cm
Q
q
1 0 0 1 62.69291 126.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs list) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 126.0236 cm
Q
q
1 0 0 1 62.69291 108.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2849) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 78.02362 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .850697 Tw (ZFS datasets are a bit different than standard filesystems and volume management tools in GNU/Linux.) Tj T* 0 Tw .137765 Tw (With the standard LVM approach, each container is limited in size, until the full volume group is filled. This) Tj T* 0 Tw ET
Q
Q
 
endstream
endobj
% 'R115': class PDFStream 
115 0 obj
% page stream
<< /Length 6763 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 717.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .209986 Tw (means that if one logical volume needs more space, you must reduce the space in another volume. While) Tj T* 0 Tw 3.633984 Tw (this is highly simplified from the partition approach, it's still not optimal. Further, this problem is) Tj T* 0 Tw 1.430514 Tw (complicated by the myriad of tools that you must use to do the task. If that's not enough, aligning the) Tj T* 0 Tw (filesystem exactly with the container can be problematic as well.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 651.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .081098 Tw (With ZFS, each filesystem is called a "dataset". Each dataset by default has full 100% access to the entire) Tj T* 0 Tw .238171 Tw (pool. Thus, if your pool is 5 TB in size, each dataset created will have access to all 5 TB. This means that) Tj T* 0 Tw 1.437045 Tw (each dataset must be aware of how much space the other datasets are storing. Also, it's important to) Tj T* 0 Tw .429985 Tw (note, that while every dataset shares the same pool, you cannot share the pool across multiple operating) Tj T* 0 Tw (systems. ZFS is not a clustered filesystem.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 609.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .60436 Tw (Datasets that reside inside other datasets are also possible. For example, maybe you have a dataset for) Tj T* 0 Tw .370574 Tw (/var. You can also have a dataset for /var/log/ and /var/cache/. These are known as nested datasets, and) Tj T* 0 Tw (have their own dataset properties, and also have full 100% access to the entire storae pool by default.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 567.0236 cm
q
BT 1 0 0 1 0 26 Tm .102126 Tw 12 TL /F1 10 Tf 0 0 0 rg (To create a dataset, use the ) Tj /F3 10 Tf (zfs create <) Tj (pool) Tj (>) Tj (/) Tj (<) Tj (dataset) Tj (> ) Tj /F1 10 Tf (command, where ) Tj /F3 10 Tf (<) Tj (pool) Tj (> ) Tj /F1 10 Tf (is the name) Tj T* 0 Tw .625777 Tw (of the pool you want to create the dataset from, and ) Tj /F3 10 Tf (<) Tj (dataset) Tj (> ) Tj /F1 10 Tf (is the dataset you wish to create. Like) Tj T* 0 Tw (so:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 533.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs create tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 513.8236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (To see the created datasets, use the ) Tj /F3 10 Tf (zfs list ) Tj /F1 10 Tf (command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 444.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 60 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F3 10 Tf 12 TL (# zfs list) Tj T* (NAME         USED  AVAIL  REFER  MOUNTPOINT) Tj T* (tank         175K  2.92G  43.4K  /tank) Tj T* (tank/store1  41.9K 2.92G  41.9K  /tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 424.6236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Should the need arise to destroy a dataset, use the ) Tj /F3 10 Tf (zfs destroy ) Tj /F1 10 Tf (command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 355.4236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 60 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F3 10 Tf 12 TL (# zfs destroy tank/store1) Tj T* (# zfs list) Tj T* (NAME         USED  AVAIL  REFER  MOUNTPOINT) Tj T* (tank         175K  2.92G  43.4K  /tank) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 322.4236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Compression) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 310.4236 cm
Q
q
1 0 0 1 62.69291 310.4236 cm
Q
q
1 0 0 1 62.69291 298.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (LZJB invented by Jeff Bonwick) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 292.4236 cm
Q
q
1 0 0 1 62.69291 280.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (lzjb) Tj /F1 10 Tf (, ) Tj /F3 10 Tf (gzip ) Tj /F1 10 Tf (1-9 and ) Tj /F3 10 Tf (zle ) Tj /F1 10 Tf (supported) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 274.4236 cm
Q
q
1 0 0 1 62.69291 262.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (ZOL 0.6.1 supports LZ4 \(fast\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 256.4236 cm
Q
q
1 0 0 1 62.69291 244.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs set compression=lzjb tank/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 244.4236 cm
Q
q
1 0 0 1 62.69291 226.4236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2878) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 172.4236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .765318 Tw (Compression in 100% transparent to the user and applications. Jeff Bonwick invented "LZJB" which is a) Tj T* 0 Tw 2.009213 Tw (variant of the Lempel-Ziv algorithm. It's fast and lightweight, and because computer processors have) Tj T* 0 Tw .629984 Tw (gotten multiple cores, and exceptionally fast, it's almost a no-brainer to enable compression. In my tests,) Tj T* 0 Tw (I've seen less than a 1% performance hit.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 118.4236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.017318 Tw (ZFS supports also supports the gzip\(1\) and ZLE algorithms. ZLE will provide the best performance, but) Tj T* 0 Tw .092765 Tw (won't give you outstanding algorithms. ZFS supports all 9 levels of gzip\(1\), with level 1 being the most fast) Tj T* 0 Tw .009535 Tw (and less tight level to level 9 being the slowest and most tight level. As of 0.6.1 with ZFS on Linux, the LZ4) Tj T* 0 Tw (algorithm is also supported, which provides amazing performance, and amazing compression ratios.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 100.4236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (To set compression on your datasets, use the ) Tj /F3 10 Tf (zfs set ) Tj /F1 10 Tf (command:) Tj T* ET
Q
Q
 
endstream
endobj
% 'R116': class PDFStream 
116 0 obj
% page stream
<< /Length 8516 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 739.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set compression=lzjb tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 695.8236 cm
q
BT 1 0 0 1 0 26 Tm .228735 Tw 12 TL /F1 10 Tf 0 0 0 rg (It's probably best practice to specify the algorithm rather than specifying ) Tj /F3 10 Tf (on ) Tj /F1 10 Tf (as the argument. The reason) Tj T* 0 Tw .119985 Tw (being, is if for any reason, the default compression algorithm is changed, the underlying blocks will remain) Tj T* 0 Tw (standardized on a single algorithm. LZJB is also probably recommended.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 662.8236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Deduplication) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 650.8236 cm
Q
q
1 0 0 1 62.69291 650.8236 cm
Q
q
1 0 0 1 62.69291 638.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Block level) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 632.8236 cm
Q
q
1 0 0 1 62.69291 620.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Deduplication table stored in RAM) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 614.8236 cm
Q
q
1 0 0 1 62.69291 602.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Occupies 25% of the ARC) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 596.8236 cm
Q
q
1 0 0 1 62.69291 584.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Rule of thumb: 5 GB of RAM for every 1 TB of disk) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 578.8236 cm
Q
q
1 0 0 1 62.69291 566.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Set on datasets, applied pool-wide) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 560.8236 cm
Q
q
1 0 0 1 62.69291 548.8236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs set dedup=on tank/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 548.8236 cm
Q
q
1 0 0 1 62.69291 530.8236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2878) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 488.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .117485 Tw (Deduplication is another way to save storage space. ZFS deduplicates data on the block level, rather than) Tj T* 0 Tw .44332 Tw (the file or byte levels. Deduplication is set per-dataset, but the data is deduplicated against all the data in) Tj T* 0 Tw (the pool. To know what data has been deduplicated, a deduplication table is stored in the ARC in RAM.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 422.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .042488 Tw (The deduplication table can be a bit unwieldly. It occupies 25% of the ARC by default, and it's a good "rule) Tj T* 0 Tw .32104 Tw (of thumb" to assume that the deduplication table needs 5 GB of ARC for every 1 TB of deduplicated disk.) Tj T* 0 Tw .478626 Tw (So, for 1 TB of disk, you should have at least 20 GB of RAM. If you don't the deduplication table will spill) Tj T* 0 Tw 2.402488 Tw (out to disk. This may not be a problem with a fast SSD or NVRAM drive, but with platter, can be) Tj T* 0 Tw (horrendous.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 368.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 3.015984 Tw (Because deduplication is checking against every block in the pool, this means that if you disable) Tj T* 0 Tw 1.05436 Tw (deduplication on a dataset, the table still must exist to manage existing deduplicated data, until all data) Tj T* 0 Tw .778735 Tw (blocks are no longer deduplicated. So, if you find that deduplication is causing problems, disabling it will) Tj T* 0 Tw (not provide immediate relief.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 350.8236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (To set deduplication on a dataset, use the ) Tj /F3 10 Tf (zfs set ) Tj /F1 10 Tf (command as well:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 317.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set dedup=on tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 284.6236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Snapshots and Clones) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 272.6236 cm
Q
q
1 0 0 1 62.69291 272.6236 cm
Q
q
1 0 0 1 62.69291 260.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (First-class read-only filesystems) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 254.6236 cm
Q
q
1 0 0 1 62.69291 242.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Store only deltas) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 236.6236 cm
Q
q
1 0 0 1 62.69291 224.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (pool@snapshot-name) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 218.6236 cm
Q
q
1 0 0 1 62.69291 206.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (pool/dataset@snapshot-name) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 200.6236 cm
Q
q
1 0 0 1 62.69291 188.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs snapshot tank/store1@001) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 182.6236 cm
Q
q
1 0 0 1 62.69291 170.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs list -t snapshot) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 164.6236 cm
Q
q
1 0 0 1 62.69291 152.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (ls /pool/dataset/.zfs/snapshot/) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 152.6236 cm
Q
q
1 0 0 1 62.69291 134.6236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2900) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 80.62362 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .537356 Tw (Think of snapshots as a digital photograph in time. When you walk out to a busy intersection, and take a) Tj T* 0 Tw 1.863876 Tw (photo with your camera, you get a snashot of what was going on at that intersection at that specific) Tj T* 0 Tw 1.793984 Tw (amount of time. Even though the intersection is constantly changing, you will always have a physical) Tj T* 0 Tw 3.07284 Tw (representation of that the data looked like at that intersection at that specific moment. Filesystem) Tj T* 0 Tw ET
Q
Q
 
endstream
endobj
% 'R117': class PDFStream 
117 0 obj
% page stream
<< /Length 6838 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (snapshots are similar in nature.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL .971098 Tw (A snapshot is a first-class filesystem. All of the data is fully accessible, and can be copied, archived, or) Tj T* 0 Tw 1.841412 Tw (sent to other computers. However, snapshots are read-only. You cannot modify data in the snopshot) Tj T* 0 Tw 1.01936 Tw (itself. You must copy the data out of the snapshot, before modification. The reason being, is snapshots) Tj T* 0 Tw .044692 Tw (store only delta changes from the time that the snapshot was taken. This means that snapshots are cheap) Tj T* 0 Tw 1.032209 Tw (in terms of storage. You colud store millions of snapshots, so long as the deltas remain small, and you) Tj T* 0 Tw (have the storage.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 645.0236 cm
q
BT 1 0 0 1 0 14 Tm 2.321751 Tw 12 TL /F1 10 Tf 0 0 0 rg (To create a snapshot of a dataset, you must give the snapshot a name. Use the ) Tj /F3 10 Tf (zfs snapshot) Tj T* 0 Tw (<) Tj (pool) Tj (>) Tj (/) Tj (<) Tj (dataset) Tj (>) Tj (@) Tj (<) Tj (name) Tj (> ) Tj /F1 10 Tf (syntax for creating your snapshot, as follows:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 611.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs snapshot tank/store1@001) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 567.8236 cm
q
BT 1 0 0 1 0 26 Tm .639461 Tw 12 TL /F1 10 Tf 0 0 0 rg (To see our snapshot, we have two ways of viewing it. We can pass the ) Tj /F3 10 Tf (-t snapshot ) Tj /F1 10 Tf (argument to the) Tj T* 0 Tw 1.338221 Tw /F3 10 Tf (zfs list ) Tj /F1 10 Tf (command, or we can cd\(1\) into the directory. Let's look at the ) Tj /F3 10 Tf (zfs list -t snapshot) Tj T* 0 Tw /F1 10 Tf (command first:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 510.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 48 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F3 10 Tf 12 TL (# zfs list -t snapshot -r tank/store1) Tj T* (NAME              USED  AVAIL  REFER  MOUNTPOINT) Tj T* (tank/store1@001      0      -   345K  -) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 478.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.091098 Tw (We can also see tho contents of the snapshot by looking in the hidden .zfs directory under the dataset) Tj T* 0 Tw (mountpoint:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 397.4236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 72 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F3 10 Tf 12 TL (# ls /tank/store1/.zfs/snapshot/) Tj T* (001/) Tj T* (# ls /tank/store1/.zfs/snapshot/001/) Tj T* (file10.img  file1.img  file2.img  file3.img  file4.img) Tj T* (file5.img   file6.img  file7.img  file8.img  file9.img) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 365.4236 cm
q
BT 1 0 0 1 0 14 Tm 1.322339 Tw 12 TL /F1 10 Tf 0 0 0 rg (By default, the ) Tj /F3 10 Tf (.zfs ) Tj /F1 10 Tf (directory is hidden, even from ls\(1\). To show the directory as a standard hidden) Tj T* 0 Tw (directory, you must set the ) Tj /F3 10 Tf (snapdir ) Tj /F1 10 Tf (dataset property to "visible":) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 332.2236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set snapdir=visible tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 312.2236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Now, if we were to change some data, our snapshot should reflect the changes:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 306.2236 cm
Q
q
1 0 0 1 62.69291 258.2236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 0 cm
q
BT 1 0 0 1 0 38 Tm 5.89872 Tw 12 TL /F1 10 Tf 0 0 0 rg (# dd if=/dev/random of=/tank/store1/file11.img bs=1 count=$RANDOM # dd if=/dev/random) Tj T* 0 Tw 1.869069 Tw (of=/tank/store1/file5.img bs=1 count=$RANDOM # zfs list -t snapshot -r tank/store1 NAME USED) Tj T* 0 Tw 1.121976 Tw (AVAIL REFER MOUNTPOINT ) Tj 0 0 .501961 rg (tank/store1@001 ) Tj 0 0 0 rg (55.9K - 345K - # ls /tank/store1/.zfs/snapshot/001/) Tj T* 0 Tw (file10.img file1.img file2.img file3.img file4.img file 5.img file6.img file7.img file8.img file9.img) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 258.2236 cm
Q
q
1 0 0 1 62.69291 228.2236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.415868 Tw (Notice that the contents of file5.img changed, so the snapshot ended up getting a copy of the original) Tj T* 0 Tw (data. However, file11.img did not exist at the time of the snapshot, so it is not part of the snapshot itself.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 198.2236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.327126 Tw (Because snapshots are first-class filesystems, they are treated as standard datasets for the most part,) Tj T* 0 Tw (and can be destroyed just like a standard ZFS dataset.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 165.2236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Sending and Receiving Snapshots) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 153.2236 cm
Q
q
1 0 0 1 62.69291 153.2236 cm
Q
q
1 0 0 1 62.69291 141.2236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Full filesystems send to STDOUT) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 135.2236 cm
Q
q
1 0 0 1 62.69291 123.2236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Redirect to image file) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 117.2236 cm
Q
q
1 0 0 1 62.69291 105.2236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Combine with OpenSSL/GnuPG) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 99.22362 cm
Q
q
1 0 0 1 62.69291 87.22362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Redirect to STDIN) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 81.22362 cm
Q
 
endstream
endobj
% 'R118': class PDFStream 
118 0 obj
% page stream
<< /Length 8036 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Receive filesystems) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 747.0236 cm
Q
q
1 0 0 1 62.69291 735.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Combine with OpenSSH) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 729.0236 cm
Q
q
1 0 0 1 62.69291 717.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs send tank/store1@001 | zfs recv backup/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 717.0236 cm
Q
q
1 0 0 1 62.69291 699.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2910) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 657.0236 cm
q
BT 1 0 0 1 0 26 Tm 2.173876 Tw 12 TL /F1 10 Tf 0 0 0 rg (ZFS has the capability to send snapshots to other ZFS pools. This gives us the ability to send full,) Tj T* 0 Tw .255697 Tw (complete filesystems to another destination, while keeping the original source filesystem intact. To send a) Tj T* 0 Tw (ZFS snapshot, we use the ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (command.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.596488 Tw (By default, it will send all of the data to STDOUT. This allows us to use the standard Unix utilities to) Tj T* 0 Tw (redirect and manipulate the data. For example, we could send the filesystem to a single monolithic image:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 593.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (# zfs send tank/store1@001 ) Tj (>) Tj ( /tank/store1/001.img) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 573.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (We could encrypt and/or compress the data using gzip\(1\), openssl\(1\) and gpg\(1\):) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 537.0359 cm
q
q
.773863 0 0 .773863 0 0 cm
q
1 0 0 1 6.6 8.528639 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 606 36 re B*
Q
q
BT 1 0 0 1 0 14 Tm 12 TL /F3 10 Tf 0 0 0 rg (# zfs send tank/store1@001 | gzip ) Tj (>) Tj ( /tank/store1/001.img.gz) Tj T* (# zfs send tank/store1@001 | gzip | openssl enc -aes-256-cbc -a -salt ) Tj (>) Tj ( /tank/store1/001.img.gz.asc) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 493.0359 cm
q
BT 1 0 0 1 0 26 Tm .24856 Tw 12 TL /F1 10 Tf 0 0 0 rg (ZFS also provides a utility to catch the stream on STDIN, and send it to a dataset in another pool with the) Tj T* 0 Tw .977633 Tw /F3 10 Tf (zfs receive ) Tj /F1 10 Tf (command. For example, if I had a pool named ) Tj /F3 10 Tf (backup) Tj /F1 10 Tf (, I could send my snapshot from) Tj T* 0 Tw (tank to backup as follows:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 459.8359 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs send tank/store1@001 | zfs recv backup/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 439.8359 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Note that the ) Tj /F3 10 Tf (recv ) Tj /F1 10 Tf (subcommand is an alias to ) Tj /F3 10 Tf (receive) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 406.8359 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (ZVOLs) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 394.8359 cm
Q
q
1 0 0 1 62.69291 394.8359 cm
Q
q
1 0 0 1 62.69291 382.8359 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Exported block device) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 376.8359 cm
Q
q
1 0 0 1 62.69291 364.8359 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Can be formatted with any filesystem) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 358.8359 cm
Q
q
1 0 0 1 62.69291 346.8359 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Useful for swap and VM storage) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 340.8359 cm
Q
q
1 0 0 1 62.69291 328.8359 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs create -V 1G tank/vol1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 322.8359 cm
Q
q
1 0 0 1 62.69291 310.8359 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (mkfs -t ext4 /dev/zvol/tank/vol1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 310.8359 cm
Q
q
1 0 0 1 62.69291 292.8359 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2933) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 238.8359 cm
q
BT 1 0 0 1 0 38 Tm 1.152927 Tw 12 TL /F1 10 Tf 0 0 0 rg (ZFS allows you to create exported block devices, as you may already be used to with ) Tj /F3 10 Tf (mdadm\(8\) ) Tj /F1 10 Tf (and) Tj T* 0 Tw .21683 Tw (LVM2. With a block device, you can do anything with it that you could do with standard block devices. For) Tj T* 0 Tw .872488 Tw (example, it could be used as a swap device. It could be used as the block device for a virtual machine.) Tj T* 0 Tw (You could even format it with ext4, and mount it. These block devices are referred to as ZVOLs.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 208.8359 cm
q
BT 1 0 0 1 0 14 Tm .160465 Tw 12 TL /F1 10 Tf 0 0 0 rg (To create a ZVOL, just pass the ) Tj /F3 10 Tf (-V ) Tj /F1 10 Tf (switch with an argument to its size. For example, if I wished to create) Tj T* 0 Tw (a 1 GB ZVOL to use for swap, I could issue the following command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 175.6359 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs create -V 1G tank/swap) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 143.6359 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.204692 Tw (To verify that I have created the block device, a file was created under /dev, with symlinks created) Tj T* 0 Tw (elsewhere:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 78.98912 cm
q
q
.773863 0 0 .773863 0 0 cm
q
1 0 0 1 6.6 8.528639 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 606 72 re B*
Q
q
BT 1 0 0 1 0 50 Tm 12 TL /F3 10 Tf 0 0 0 rg (# find /dev -ls | grep zd) Tj T* (476149    0 lrwxrwxrwx   1 root     root            9 Feb 22 08:17 /dev/zvol/tank/swap -) Tj (>) Tj ( ../../zd0) Tj T* (476144    0 lrwxrwxrwx   1 root     root            6 Feb 22 08:17 /dev/tank/swap -) Tj (>) Tj ( ../zd0) Tj T* (476138    0 brw-rw---T   1 root     disk              Feb 22 08:17 /dev/zd0) Tj T* (476142    0 lrwxrwxrwx   1 root     root            6 Feb 22 08:17 /dev/block/230:0 -) Tj (>) Tj ( ../zd0) Tj T* ET
Q
Q
Q
Q
Q
 
endstream
endobj
% 'R119': class PDFStream 
119 0 obj
% page stream
<< /Length 7416 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .379535 Tw (Now that we have our swap device, let's set it up. First we must format it as a swap device, so the kernel) Tj T* 0 Tw (knows it's available, then we must "turn it on":) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 695.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 36 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F3 10 Tf 12 TL (# mkswap /dev/zd0) Tj T* (# swapon /dev/zd0) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 651.8236 cm
q
BT 1 0 0 1 0 26 Tm .566655 Tw 12 TL /F1 10 Tf 0 0 0 rg (We can now verify that we have an extra 1 GB of swap for the kernel to use. When using swap devices,) Tj T* 0 Tw .334104 Tw (it's best practice to disable synchronous writes, to make "the swap of death" as painless as possible. Use) Tj T* 0 Tw /F3 10 Tf (zfs set ) Tj /F1 10 Tf (to enable that:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 618.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set sync=disabled tank/swap) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 574.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.005777 Tw (Unfortunately with ZFS on Linux, as of 0.6.1, there is a bug with VZOLs where they are not created on) Tj T* 0 Tw .622988 Tw (boot. There is a work around until the bug is fixed, which is to rename the ZVOL twice. Put the following) Tj T* 0 Tw (into your /etc/rc.local:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 568.6236 cm
Q
q
1 0 0 1 62.69291 556.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (zfs rename tank/swap tank/foo zfs rename tank/foo tank/swap swapon tank/swap) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 556.6236 cm
Q
q
1 0 0 1 62.69291 538.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Now your swap ZVOL will be ready for use.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 508.6236 cm
q
BT 1 0 0 1 0 14 Tm .344988 Tw 12 TL /F1 10 Tf 0 0 0 rg (This is only one example of how to use a ZVOL. As mentioned, you could use them for storage for virtual) Tj T* 0 Tw (machines, you could format them using any standard filesystem utility, such as ) Tj /F3 10 Tf (mkfs\(8\)) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 475.6236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Sharing via NFS, SMB and iSCSI) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 463.6236 cm
Q
q
1 0 0 1 62.69291 463.6236 cm
Q
q
1 0 0 1 62.69291 451.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Native support) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 445.6236 cm
Q
q
1 0 0 1 62.69291 433.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Requires running daemon) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 427.6236 cm
Q
q
1 0 0 1 62.69291 415.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Benefit: dataset mounted, before shared) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 409.6236 cm
Q
q
1 0 0 1 62.69291 397.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Full NFS option support) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 391.6236 cm
Q
q
1 0 0 1 62.69291 379.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (SMB support spotty for GNU/Linux) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 373.6236 cm
Q
q
1 0 0 1 62.69291 361.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (No iSCSI support for GNU/Linux... yet) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 355.6236 cm
Q
q
1 0 0 1 62.69291 343.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (zfs set sharenfs|sharesmb|shareiscsi tank/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 343.6236 cm
Q
q
1 0 0 1 62.69291 325.6236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2943) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 259.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .446654 Tw (Native NFS, Samba \(CIFS\) and iSCSI support are built into ZFS. The largest reason for this, is to enable) Tj T* 0 Tw .396235 Tw (an order of operational consistency. For example, you want to make sure your dataset is mounted before) Tj T* 0 Tw .59061 Tw (your export your directory over NFS. You don't want NFS clients sending data to the export prematurely.) Tj T* 0 Tw .321654 Tw (Thus, the ZFS dataset will be mounted before the export is available on the network, this NFS clients can) Tj T* 0 Tw (safely send data to it.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 217.6236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .107209 Tw (For NFS, full NFS option support is builtin. However, it does require a running NFS daemon on the server.) Tj T* 0 Tw 2.387318 Tw (With Debian and Ubuntu GNU/Linux, the exports\(5\) file must be created before you share the ZFS) Tj T* 0 Tw (datasets via NFS.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 163.6236 cm
q
BT 1 0 0 1 0 38 Tm .323145 Tw 12 TL /F1 10 Tf 0 0 0 rg (To enable NFS support for your dataset, just use the ) Tj /F3 10 Tf (zfs set ) Tj /F1 10 Tf (command. Because all of the full NFS v3) Tj T* 0 Tw 1.320574 Tw (options are supported, you can pass them as arguments to the command. For example, if I wanted to) Tj T* 0 Tw .346651 Tw (make the /tank/share/ directory available only to the 10.80.86.0/24 network, then I could use the following) Tj T* 0 Tw (command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 130.4236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set sharenfs="rw=10.80.86.0/24" tank/share) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 86.42362 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 2.15436 Tw (SMB support is also available, although I've had trouble getting it working correctly. As with NFS, it) Tj T* 0 Tw .176457 Tw (requires a running SMB daemon on the server. To enable SMB sharing on your dataset, use the following) Tj T* 0 Tw (command:) Tj T* ET
Q
Q
 
endstream
endobj
% 'R120': class PDFStream 
120 0 obj
% page stream
<< /Length 5769 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 739.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set sharesmb=on tank/share) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 695.8236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .292209 Tw (iSCSI support is also available with Solaris and the Illumos builds. It is currently not supported for ZFS on) Tj T* 0 Tw .72332 Tw (Linux, although it is pending, and we will likely see it shortly. To enable iSCSI support, use the following) Tj T* 0 Tw (command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 662.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set shareiscsi=on tank/share) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 629.6236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Dataset Properties) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 617.6236 cm
Q
q
1 0 0 1 62.69291 617.6236 cm
Q
q
1 0 0 1 62.69291 605.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F3 10 Tf 0 0 0 rg (compression ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (compressratio) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 599.6236 cm
Q
q
1 0 0 1 62.69291 587.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (mountpoint) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 581.6236 cm
Q
q
1 0 0 1 62.69291 569.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (snapdir) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 563.6236 cm
Q
q
1 0 0 1 62.69291 551.6236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (sync) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 551.6236 cm
Q
q
1 0 0 1 62.69291 533.6236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2950) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 479.6236 cm
q
BT 1 0 0 1 0 38 Tm .78332 Tw 12 TL /F1 10 Tf 0 0 0 rg (ZFS datasets also contain properties, as ZFS pools do. In fact, they have a great deal more. And, there) Tj T* 0 Tw .111751 Tw (are properties that are available for snapshots that are not available for datasets, and vice versa. A similar) Tj T* 0 Tw 3.027485 Tw (command for ) Tj /F3 10 Tf (zfs\(8\) ) Tj /F1 10 Tf (is used to list the properties like with ) Tj /F3 10 Tf (zpool\(8\)) Tj /F1 10 Tf (. Just use the ) Tj /F3 10 Tf (zfs get) Tj T* 0 Tw /F1 10 Tf (command. As with ) Tj /F3 10 Tf (zpool\(8\)) Tj /F1 10 Tf (, you can list all properties, or list them one at a time, comma separated:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 86.42362 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 384 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 362 Tm /F3 10 Tf 12 TL (# zfs get all tank/store1) Tj T* (NAME         PROPERTY              VALUE                  SOURCE) Tj T* (tank/store1  type                  filesystem             -) Tj T* (tank/store1  creation              Fri Feb 22  7:57 2013  -) Tj T* (tank/store1  used                  352K                   -) Tj T* (tank/store1  available             69.7G                  -) Tj T* (tank/store1  referenced            279K                   -) Tj T* (tank/store1  compressratio         1.00x                  -) Tj T* (tank/store1  mounted               yes                    -) Tj T* (tank/store1  quota                 none                   default) Tj T* (tank/store1  reservation           none                   default) Tj T* (tank/store1  recordsize            128K                   default) Tj T* (tank/store1  mountpoint            /tank/store1           default) Tj T* (tank/store1  sharenfs              off                    default) Tj T* (tank/store1  checksum              on                     default) Tj T* (tank/store1  compression           off                    default) Tj T* (tank/store1  atime                 on                     default) Tj T* (tank/store1  devices               on                     default) Tj T* (tank/store1  exec                  on                     default) Tj T* (tank/store1  setuid                on                     default) Tj T* (tank/store1  readonly              off                    default) Tj T* (tank/store1  zoned                 off                    default) Tj T* (tank/store1  snapdir               hidden                 default) Tj T* (tank/store1  aclinherit            restricted             default) Tj T* (tank/store1  canmount              on                     default) Tj T* (tank/store1  xattr                 on                     default) Tj T* (tank/store1  copies                1                      default) Tj T* (tank/store1  version               5                      -) Tj T* (tank/store1  utf8only              off                    -) Tj T* (tank/store1  normalization         none                   -) Tj T* (tank/store1  casesensitivity       sensitive              -) Tj T* ET
Q
Q
Q
Q
Q
 
endstream
endobj
% 'R121': class PDFStream 
121 0 obj
% page stream
<< /Length 6640 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 547.8236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 216 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 194 Tm /F3 10 Tf 12 TL (tank/store1  vscan                 off                    default) Tj T* (tank/store1  nbmand                off                    default) Tj T* (tank/store1  sharesmb              off                    default) Tj T* (tank/store1  refquota              none                   default) Tj T* (tank/store1  refreservation        none                   default) Tj T* (tank/store1  primarycache          all                    default) Tj T* (tank/store1  secondarycache        all                    default) Tj T* (tank/store1  usedbysnapshots       72.5K                  -) Tj T* (tank/store1  usedbydataset         279K                   -) Tj T* (tank/store1  usedbychildren        0                      -) Tj T* (tank/store1  usedbyrefreservation  0                      -) Tj T* (tank/store1  logbias               latency                default) Tj T* (tank/store1  dedup                 off                    default) Tj T* (tank/store1  mlslabel              none                   default) Tj T* (tank/store1  sync                  standard               default) Tj T* (tank/store1  refcompressratio      1.00x                  -) Tj T* (tank/store1  written               86.4K                  -) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 515.8236 cm
q
BT 1 0 0 1 0 14 Tm .272488 Tw 12 TL /F1 10 Tf 0 0 0 rg (All of the properties can be found on my blog post above, in the ) Tj /F3 10 Tf (zfs\(8\) ) Tj /F1 10 Tf (manual, or at the official Solaris) Tj T* 0 Tw (documentation.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 485.8236 cm
q
BT 1 0 0 1 0 14 Tm 1.648651 Tw 12 TL /F1 10 Tf 0 0 0 rg (If you wish to see only the ) Tj /F3 10 Tf (compressratio ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (mountpoint ) Tj /F1 10 Tf (properties, then you could issue the) Tj T* 0 Tw (following command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 428.6236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 48 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F3 10 Tf 12 TL (# zfs get compressratio,mountpoint tank/store1) Tj T* (tank/store1  compressratio  1.00x         -) Tj T* (tank/store1  mountpoint     /tank/store1  default) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 384.6236 cm
q
BT 1 0 0 1 0 26 Tm .062927 Tw 12 TL /F1 10 Tf 0 0 0 rg (As with the ) Tj /F3 10 Tf (zpool\(8\) ) Tj /F1 10 Tf (command, you can set properties using the ) Tj /F3 10 Tf (zfs set ) Tj /F1 10 Tf (command. For example, if I) Tj T* 0 Tw 2.20186 Tw (wished to set the compression algorithm to LZJB for the tank/store1 dataset, then I could issue the) Tj T* 0 Tw (following command:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 351.4236 cm
q
q
1 0 0 1 0 0 cm
q
1 0 0 1 6.6 6.6 cm
q
.662745 .662745 .662745 RG
.5 w
.960784 .960784 .862745 rg
n -6 -6 468.6898 24 re B*
Q
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F3 10 Tf 12 TL (# zfs set compression=lzjb tank/store1) Tj T* ET
Q
Q
Q
Q
Q
q
1 0 0 1 62.69291 295.4236 cm
q
BT 1 0 0 1 0 38 Tm .939269 Tw 12 TL /F1 10 Tf 0 0 0 rg (A note about nested filesystems. Technically speaking, ) Tj /F3 10 Tf (tank/store1 ) Tj /F1 10 Tf (is a nested dataset to ) Tj /F3 10 Tf (tank) Tj /F1 10 Tf (. As) Tj T* 0 Tw .845984 Tw (such, any properties set on the parent ) Tj /F3 10 Tf (tank ) Tj /F1 10 Tf (dataset will get inherited onto the ) Tj /F3 10 Tf (tank/store1 ) Tj /F1 10 Tf (dataset.) Tj T* 0 Tw 2.290697 Tw (Thus, if the LZJB compression algorithm was set on ) Tj /F3 10 Tf (tank) Tj /F1 10 Tf (, then when creating ) Tj /F3 10 Tf (tank/store1 ) Tj /F1 10 Tf (and) Tj T* 0 Tw /F3 10 Tf (tank/store1/cache) Tj /F1 10 Tf (, these datasets would also inherit the LZJB compression algorithm.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 265.4236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .500574 Tw (Further, if you make a change on a parent dataset, after the nested datasets have already been created,) Tj T* 0 Tw (they will automatically get ineherited down as follows:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 259.4236 cm
Q
q
1 0 0 1 62.69291 211.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 2.779147 Tw (# zfs set compression=lzjb tank # zfs get compression tank/store1 NAME PROPERTY VALUE) Tj T* 0 Tw .436412 Tw (SOURCE tank/store1 compression lzjb inherited from tank # zfs set compression=gzip tank # zfs get) Tj T* 0 Tw 4.69247 Tw (compression tank/store1 NAME PROPERTY VALUE SOURCE tank/store1 compression gzip) Tj T* 0 Tw (inherited from tank) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 211.4236 cm
Q
q
1 0 0 1 62.69291 193.4236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (If you wish to clear a nested dataset to it's parent value, then you can use the ) Tj /F3 10 Tf (zfs inherit ) Tj /F1 10 Tf (command.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 187.4236 cm
Q
q
1 0 0 1 62.69291 175.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (# zfs inherit compression tank/store1) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 175.4236 cm
Q
q
1 0 0 1 62.69291 142.4236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Dataset Best Practices and Caveats) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 130.4236 cm
Q
q
1 0 0 1 62.69291 130.4236 cm
Q
q
1 0 0 1 62.69291 118.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Enable compression by default) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 112.4236 cm
Q
q
1 0 0 1 62.69291 100.4236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Snapshot frequently and regularly) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 94.42362 cm
Q
q
1 0 0 1 62.69291 82.42362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (With sending snapshots, use incremental sends) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 76.86614 cm
Q
 
endstream
endobj
% 'R122': class PDFStream 
122 0 obj
% page stream
<< /Length 10477 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (Set ) Tj /F3 10 Tf (options zfs zfs_arc_max=2147483648 ) Tj /F1 10 Tf (for GNU/Linux) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 753.0236 cm
Q
q
1 0 0 1 62.69291 735.0236 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/?p=2963) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .907765 Tw (Again, as with the ZFS pool best practices and caveats, take this with some weight, but realize that not) Tj T* 0 Tw (everything may fit your specific situation. Try to adhere to them as best you can.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 699.0236 cm
Q
q
1 0 0 1 62.69291 699.0236 cm
Q
q
1 0 0 1 62.69291 675.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.66284 Tw (Always enable compression. There is almost certainly no reason to keep it disabled. It hardly) Tj T* 0 Tw (touches the CPU and hardly touches throughput to the drive, yet the benefits are amazing.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 669.0236 cm
Q
q
1 0 0 1 62.69291 645.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.041647 Tw (Unless you have the RAM, avoid using deduplication. Unlike compression, deduplication is very) Tj T* 0 Tw (costly on the system. The deduplication table consumes massive amounts of RAM.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 639.0236 cm
Q
q
1 0 0 1 62.69291 615.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .707045 Tw (Avoid running a ZFS root filesystem on GNU/Linux for the time being. It's a bit too experimental for) Tj T* 0 Tw (/boot and GRUB. However, do create datasets for /home/, /var/log/ and /var/cache/.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 609.0236 cm
Q
q
1 0 0 1 62.69291 585.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.269269 Tw (Snapshot frequently and regularly. Snapshots are cheap, and can keep a plethora of file versions) Tj T* 0 Tw (over time. Consider using something like the zfs-auto-snapshot script.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 579.0236 cm
Q
q
1 0 0 1 62.69291 555.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm .00881 Tw 12 TL /F1 10 Tf 0 0 0 rg (Snapqhots are not a backup. Use ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (zfs receive ) Tj /F1 10 Tf (to send your ZFS snapshots to an) Tj T* 0 Tw (external storage.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 525.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.701163 Tw (If using NFS, use ZFS NFS rather than your native exports. This can ensure that the dataset is) Tj T* 0 Tw (mounted and online before NFS clients begin sending data to the mountpoint.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 519.0236 cm
Q
q
1 0 0 1 62.69291 507.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Don't mix NFS kernel exports and ZFS NFS exports. This is difficult to administer and maintain.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 501.0236 cm
Q
q
1 0 0 1 62.69291 477.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 6.208314 Tw (For /home/ ZFS installations, setting up nested datasets for each user. For example,) Tj T* 0 Tw (pool/home/atoponce and pool/home/dobbs. Consider using quotas on the datasets.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 471.0236 cm
Q
q
1 0 0 1 62.69291 447.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm 1.612126 Tw 12 TL /F1 10 Tf 0 0 0 rg (When using ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (and ) Tj /F3 10 Tf (zfs receive) Tj /F1 10 Tf (, send incremental streams with the ) Tj /F3 10 Tf (zfs send -i) Tj T* 0 Tw /F1 10 Tf (switch. This can be an exceptional time saver.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 441.0236 cm
Q
q
1 0 0 1 62.69291 417.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm 1.81152 Tw 12 TL /F1 10 Tf 0 0 0 rg (Consider using ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (over ) Tj /F3 10 Tf (rsync\(1\)) Tj /F1 10 Tf (, as the ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (command can preserve dataset) Tj T* 0 Tw (properties.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 417.0236 cm
Q
q
1 0 0 1 62.69291 399.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (And the caveats that you should be aware of:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 393.0236 cm
Q
q
1 0 0 1 62.69291 393.0236 cm
Q
q
1 0 0 1 62.69291 345.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm .76311 Tw 12 TL /F1 10 Tf 0 0 0 rg (A ) Tj /F3 10 Tf (zfs destroy ) Tj /F1 10 Tf (can cause downtime for other datasets. A ) Tj /F3 10 Tf (zfs destroy ) Tj /F1 10 Tf (will touch every file in) Tj T* 0 Tw .058409 Tw (the dataset that resides in the storage pool. The larger the dataset, the longer this will take, and it will) Tj T* 0 Tw .290941 Tw (use all the possible IOPS out of your drives to make it happen. Thus, if it take 2 hours to destroy the) Tj T* 0 Tw (dataset, that's 2 hours of potential downtime for the other datasets in the pool.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
Q
q
1 0 0 1 62.69291 303.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .406235 Tw 12 TL /F1 10 Tf 0 0 0 rg (Debian and Ubuntu will not start the NFS daemon without a valid export in the ) Tj /F3 10 Tf (/etc/exports ) Tj /F1 10 Tf (file.) Tj T* 0 Tw .046235 Tw (You must either modify the ) Tj /F3 10 Tf (/etc/init.d/nfs ) Tj /F1 10 Tf (init script to start without an export, or create a local) Tj T* 0 Tw (dummy export.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 297.0236 cm
Q
q
1 0 0 1 62.69291 249.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm .371318 Tw 12 TL /F1 10 Tf 0 0 0 rg (Debian and Ubuntu, and probably other systems use a parallized boot. As such, init script execution) Tj T* 0 Tw .484651 Tw (order is no longer prioritized. This creates problems for mounting ZFS datasets on boot. For Debian) Tj T* 0 Tw 2.708976 Tw (and Ubuntu, touch the ) Tj /F3 10 Tf (/etc/init.d/.legacy-bootordering ) Tj /F1 10 Tf (file, and make sure that the) Tj T* 0 Tw /F3 10 Tf (/etc/init.d/zfs ) Tj /F1 10 Tf (init script is the first to start, before all other services in that runlevel.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 243.0236 cm
Q
q
1 0 0 1 62.69291 219.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.45528 Tw (Do not create ZFS storage pools from files in other ZFS datasets. This will cause all sorts of) Tj T* 0 Tw (headaches and problems.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 213.0236 cm
Q
q
1 0 0 1 62.69291 177.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .58683 Tw (When creating ZVOLs, make sure to set the block size as the same, or a multiple, of the block size) Tj T* 0 Tw .81881 Tw (that you will be formatting the ZVOL with. If the block sizes do not align, performance issues could) Tj T* 0 Tw (arise.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 171.0236 cm
Q
q
1 0 0 1 62.69291 111.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 45 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 50 Tm .027765 Tw 12 TL /F1 10 Tf 0 0 0 rg (When loading the ) Tj /F3 10 Tf (zfs ) Tj /F1 10 Tf (kernel module, make sure to set a maximum number for the ARC. Doing a lot) Tj T* 0 Tw 1.479431 Tw (of ) Tj /F3 10 Tf (zfs send ) Tj /F1 10 Tf (or snapshot operations will cache the data. If not set, RAM will slowly fill until the) Tj T* 0 Tw 6.887674 Tw (kernel invokes OOM killer, and the system becomes responsive. I have set in my) Tj T* 0 Tw .456342 Tw (/etc/modprobe.d/zfs.conf file ) Tj /F3 10 Tf (options zfs zfs_arc_max=2147483648) Tj /F1 10 Tf (, which is a 2 GB limit for) Tj T* 0 Tw (the ARC.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 111.0236 cm
Q
q
1 0 0 1 62.69291 78.02362 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Fin) Tj T* ET
Q
Q
 
endstream
endobj
% 'R123': class PDFStream 
123 0 obj
% page stream
<< /Length 2340 >>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 759.0236 cm
Q
q
1 0 0 1 62.69291 759.0236 cm
Q
q
1 0 0 1 62.69291 747.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Questions?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 741.0236 cm
Q
q
1 0 0 1 62.69291 729.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Comments?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 723.0236 cm
Q
q
1 0 0 1 62.69291 711.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Rude Remarks?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 711.0236 cm
Q
q
1 0 0 1 62.69291 678.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Resources) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 666.0236 cm
Q
q
1 0 0 1 62.69291 666.0236 cm
Q
q
1 0 0 1 62.69291 654.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://pthree.org/category/zfs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 648.0236 cm
Q
q
1 0 0 1 62.69291 636.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (http://aarontoponce.org/presents/zfs) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 630.0236 cm
Q
q
1 0 0 1 62.69291 618.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (aaron.toponce@gmail.com) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 618.0236 cm
Q
 
endstream
endobj
% 'R124': class PDFPageLabels 
124 0 obj
% Document Root
<< /Nums [ 0
 125 0 R
 1
 126 0 R
 2
 127 0 R
 3
 128 0 R
 4
 129 0 R
 5
 130 0 R
 6
 131 0 R
 7
 132 0 R
 8
 133 0 R
 9
 134 0 R
 10
 135 0 R
 11
 136 0 R
 12
 137 0 R
 13
 138 0 R
 14
 139 0 R
 15
 140 0 R
 16
 141 0 R
 17
 142 0 R
 18
 143 0 R
 19
 144 0 R
 20
 145 0 R
 21
 146 0 R
 22
 147 0 R
 23
 148 0 R
 24
 149 0 R ] >>
endobj
% 'R125': class PDFPageLabel 
125 0 obj
% None
<< /S /D
 /St 1 >>
endobj
% 'R126': class PDFPageLabel 
126 0 obj
% None
<< /S /D
 /St 2 >>
endobj
% 'R127': class PDFPageLabel 
127 0 obj
% None
<< /S /D
 /St 3 >>
endobj
% 'R128': class PDFPageLabel 
128 0 obj
% None
<< /S /D
 /St 4 >>
endobj
% 'R129': class PDFPageLabel 
129 0 obj
% None
<< /S /D
 /St 5 >>
endobj
% 'R130': class PDFPageLabel 
130 0 obj
% None
<< /S /D
 /St 6 >>
endobj
% 'R131': class PDFPageLabel 
131 0 obj
% None
<< /S /D
 /St 7 >>
endobj
% 'R132': class PDFPageLabel 
132 0 obj
% None
<< /S /D
 /St 8 >>
endobj
% 'R133': class PDFPageLabel 
133 0 obj
% None
<< /S /D
 /St 9 >>
endobj
% 'R134': class PDFPageLabel 
134 0 obj
% None
<< /S /D
 /St 10 >>
endobj
% 'R135': class PDFPageLabel 
135 0 obj
% None
<< /S /D
 /St 11 >>
endobj
% 'R136': class PDFPageLabel 
136 0 obj
% None
<< /S /D
 /St 12 >>
endobj
% 'R137': class PDFPageLabel 
137 0 obj
% None
<< /S /D
 /St 13 >>
endobj
% 'R138': class PDFPageLabel 
138 0 obj
% None
<< /S /D
 /St 14 >>
endobj
% 'R139': class PDFPageLabel 
139 0 obj
% None
<< /S /D
 /St 15 >>
endobj
% 'R140': class PDFPageLabel 
140 0 obj
% None
<< /S /D
 /St 16 >>
endobj
% 'R141': class PDFPageLabel 
141 0 obj
% None
<< /S /D
 /St 17 >>
endobj
% 'R142': class PDFPageLabel 
142 0 obj
% None
<< /S /D
 /St 18 >>
endobj
% 'R143': class PDFPageLabel 
143 0 obj
% None
<< /S /D
 /St 19 >>
endobj
% 'R144': class PDFPageLabel 
144 0 obj
% None
<< /S /D
 /St 20 >>
endobj
% 'R145': class PDFPageLabel 
145 0 obj
% None
<< /S /D
 /St 21 >>
endobj
% 'R146': class PDFPageLabel 
146 0 obj
% None
<< /S /D
 /St 22 >>
endobj
% 'R147': class PDFPageLabel 
147 0 obj
% None
<< /S /D
 /St 23 >>
endobj
% 'R148': class PDFPageLabel 
148 0 obj
% None
<< /S /D
 /St 24 >>
endobj
% 'R149': class PDFPageLabel 
149 0 obj
% None
<< /S /D
 /St 25 >>
endobj
xref
0 150
0000000000 65535 f
0000000113 00000 n
0000000234 00000 n
0000000399 00000 n
0000000586 00000 n
0000000834 00000 n
0000001088 00000 n
0000001336 00000 n
0000001600 00000 n
0000001873 00000 n
0000002100 00000 n
0000002454 00000 n
0000002686 00000 n
0000003002 00000 n
0000003271 00000 n
0000003445 00000 n
0000003675 00000 n
0000004001 00000 n
0000004242 00000 n
0000004472 00000 n
0000004798 00000 n
0000005028 00000 n
0000005330 00000 n
0000005611 00000 n
0000005907 00000 n
0000006137 00000 n
0000006454 00000 n
0000006699 00000 n
0000006929 00000 n
0000007255 00000 n
0000007486 00000 n
0000007803 00000 n
0000008048 00000 n
0000008279 00000 n
0000008605 00000 n
0000008836 00000 n
0000009139 00000 n
0000009435 00000 n
0000009666 00000 n
0000009969 00000 n
0000010265 00000 n
0000010496 00000 n
0000010813 00000 n
0000011044 00000 n
0000011361 00000 n
0000011606 00000 n
0000011837 00000 n
0000012163 00000 n
0000012391 00000 n
0000012708 00000 n
0000012953 00000 n
0000013184 00000 n
0000013510 00000 n
0000013741 00000 n
0000014058 00000 n
0000014289 00000 n
0000014592 00000 n
0000014888 00000 n
0000015119 00000 n
0000015436 00000 n
0000015686 00000 n
0000015942 00000 n
0000016178 00000 n
0000016499 00000 n
0000016659 00000 n
0000016928 00000 n
0000017054 00000 n
0000017234 00000 n
0000017417 00000 n
0000017603 00000 n
0000017791 00000 n
0000017980 00000 n
0000018183 00000 n
0000018371 00000 n
0000018560 00000 n
0000018741 00000 n
0000018927 00000 n
0000019113 00000 n
0000019301 00000 n
0000019490 00000 n
0000019678 00000 n
0000019887 00000 n
0000020096 00000 n
0000020293 00000 n
0000020493 00000 n
0000020694 00000 n
0000020887 00000 n
0000021096 00000 n
0000021294 00000 n
0000021482 00000 n
0000021672 00000 n
0000021869 00000 n
0000022077 00000 n
0000022259 00000 n
0000022466 00000 n
0000022661 00000 n
0000022872 00000 n
0000023052 00000 n
0000023206 00000 n
0000023531 00000 n
0000032350 00000 n
0000043300 00000 n
0000051195 00000 n
0000059934 00000 n
0000067725 00000 n
0000074735 00000 n
0000079395 00000 n
0000087273 00000 n
0000095829 00000 n
0000103212 00000 n
0000109869 00000 n
0000116019 00000 n
0000120517 00000 n
0000130725 00000 n
0000140728 00000 n
0000151392 00000 n
0000158256 00000 n
0000166873 00000 n
0000173812 00000 n
0000181949 00000 n
0000189466 00000 n
0000195336 00000 n
0000202077 00000 n
0000212656 00000 n
0000215101 00000 n
0000215548 00000 n
0000215627 00000 n
0000215706 00000 n
0000215785 00000 n
0000215864 00000 n
0000215943 00000 n
0000216022 00000 n
0000216101 00000 n
0000216180 00000 n
0000216259 00000 n
0000216339 00000 n
0000216419 00000 n
0000216499 00000 n
0000216579 00000 n
0000216659 00000 n
0000216739 00000 n
0000216819 00000 n
0000216899 00000 n
0000216979 00000 n
0000217059 00000 n
0000217139 00000 n
0000217219 00000 n
0000217299 00000 n
0000217379 00000 n
0000217459 00000 n
trailer
<< /ID 
 % ReportLab generated PDF document -- digest (http://www.reportlab.com) 
 [(vd\233K+\331Z\014\332\004\243\013m\017f<) (vd\233K+\331Z\014\332\004\243\013m\017f<)] 

 /Info 64 0 R
 /Root 63 0 R
 /Size 150 >>
startxref
217508
%%EOF
